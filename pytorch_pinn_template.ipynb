{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch PINN Template\n",
    "\n",
    "This notebook provides a comprehensive template for implementing Physics-Informed Neural Networks (PINNs) using pure PyTorch, replacing the DeepXDE-based implementations in the course.\n",
    "\n",
    "## Key Differences from DeepXDE:\n",
    "- Manual implementation of automatic differentiation for PDE residuals\n",
    "- Custom sampling of collocation points and boundary points\n",
    "- Explicit loss function construction and weighting\n",
    "- More control over training process and optimization strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Architecture\n",
    "\n",
    "Define the neural network that will approximate the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"Physics-Informed Neural Network\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_hidden_layers=3, activation=nn.Tanh()):\n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        \n",
    "        # Build the network\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_hidden_layers - 1):\n",
    "            layers.append(activation)\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        layers.append(activation)\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using Xavier/Glorot initialization\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Example instantiation\n",
    "model = PINN(input_dim=2, hidden_dim=50, output_dim=1, num_hidden_layers=3).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Domain Sampling\n",
    "\n",
    "Functions to sample points from different parts of the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_domain_2d(bounds, num_points, device=device):\n",
    "    \"\"\"Sample points uniformly from a 2D rectangular domain.\n",
    "    \n",
    "    Args:\n",
    "        bounds: [[x_min, x_max], [y_min, y_max]]\n",
    "        num_points: Number of points to sample\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (num_points, 2)\n",
    "    \"\"\"\n",
    "    x = torch.rand(num_points, 1, device=device) * (bounds[0][1] - bounds[0][0]) + bounds[0][0]\n",
    "    y = torch.rand(num_points, 1, device=device) * (bounds[1][1] - bounds[1][0]) + bounds[1][0]\n",
    "    return torch.cat([x, y], dim=1)\n",
    "\n",
    "def sample_boundary_2d(bounds, num_points_per_side, device=device):\n",
    "    \"\"\"Sample points from the boundary of a 2D rectangular domain.\n",
    "    \n",
    "    Args:\n",
    "        bounds: [[x_min, x_max], [y_min, y_max]]\n",
    "        num_points_per_side: Number of points per boundary side\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (4 * num_points_per_side, 2)\n",
    "    \"\"\"\n",
    "    x_min, x_max = bounds[0]\n",
    "    y_min, y_max = bounds[1]\n",
    "    \n",
    "    # Left boundary (x = x_min)\n",
    "    left_y = torch.rand(num_points_per_side, 1, device=device) * (y_max - y_min) + y_min\n",
    "    left_x = torch.full_like(left_y, x_min)\n",
    "    left = torch.cat([left_x, left_y], dim=1)\n",
    "    \n",
    "    # Right boundary (x = x_max)\n",
    "    right_y = torch.rand(num_points_per_side, 1, device=device) * (y_max - y_min) + y_min\n",
    "    right_x = torch.full_like(right_y, x_max)\n",
    "    right = torch.cat([right_x, right_y], dim=1)\n",
    "    \n",
    "    # Bottom boundary (y = y_min)\n",
    "    bottom_x = torch.rand(num_points_per_side, 1, device=device) * (x_max - x_min) + x_min\n",
    "    bottom_y = torch.full_like(bottom_x, y_min)\n",
    "    bottom = torch.cat([bottom_x, bottom_y], dim=1)\n",
    "    \n",
    "    # Top boundary (y = y_max)\n",
    "    top_x = torch.rand(num_points_per_side, 1, device=device) * (x_max - x_min) + x_min\n",
    "    top_y = torch.full_like(top_x, y_max)\n",
    "    top = torch.cat([top_x, top_y], dim=1)\n",
    "    \n",
    "    return torch.cat([left, right, bottom, top], dim=0)\n",
    "\n",
    "def sample_time_domain(x_bounds, t_bounds, num_domain, num_boundary, num_initial, device=device):\n",
    "    \"\"\"Sample points for time-dependent problems.\n",
    "    \n",
    "    Args:\n",
    "        x_bounds: [x_min, x_max]\n",
    "        t_bounds: [t_min, t_max]\n",
    "        num_domain: Number of collocation points in the domain\n",
    "        num_boundary: Number of boundary points\n",
    "        num_initial: Number of initial condition points\n",
    "        device: PyTorch device\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing 'domain', 'boundary', and 'initial' points\n",
    "    \"\"\"\n",
    "    # Domain points (interior)\n",
    "    x_domain = torch.rand(num_domain, 1, device=device) * (x_bounds[1] - x_bounds[0]) + x_bounds[0]\n",
    "    t_domain = torch.rand(num_domain, 1, device=device) * (t_bounds[1] - t_bounds[0]) + t_bounds[0]\n",
    "    domain_points = torch.cat([x_domain, t_domain], dim=1)\n",
    "    \n",
    "    # Boundary points (spatial boundaries at different times)\n",
    "    t_boundary = torch.rand(num_boundary, 1, device=device) * (t_bounds[1] - t_bounds[0]) + t_bounds[0]\n",
    "    x_boundary_left = torch.full((num_boundary//2, 1), x_bounds[0], device=device)\n",
    "    x_boundary_right = torch.full((num_boundary//2, 1), x_bounds[1], device=device)\n",
    "    x_boundary = torch.cat([x_boundary_left, x_boundary_right], dim=0)\n",
    "    boundary_points = torch.cat([x_boundary, t_boundary[:num_boundary]], dim=1)\n",
    "    \n",
    "    # Initial condition points (t = t_min)\n",
    "    x_initial = torch.rand(num_initial, 1, device=device) * (x_bounds[1] - x_bounds[0]) + x_bounds[0]\n",
    "    t_initial = torch.full((num_initial, 1), t_bounds[0], device=device)\n",
    "    initial_points = torch.cat([x_initial, t_initial], dim=1)\n",
    "    \n",
    "    return {\n",
    "        'domain': domain_points,\n",
    "        'boundary': boundary_points,\n",
    "        'initial': initial_points\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "bounds = [[-1, 1], [-1, 1]]\n",
    "domain_points = sample_domain_2d(bounds, 1000)\n",
    "boundary_points = sample_boundary_2d(bounds, 50)\n",
    "\n",
    "print(f\"Domain points shape: {domain_points.shape}\")\n",
    "print(f\"Boundary points shape: {boundary_points.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic Differentiation Utilities\n",
    "\n",
    "Helper functions to compute derivatives using PyTorch's automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(u, x, create_graph=True):\n",
    "    \"\"\"Compute gradient of u with respect to x.\n",
    "    \n",
    "    Args:\n",
    "        u: Output tensor of shape (batch_size, output_dim)\n",
    "        x: Input tensor of shape (batch_size, input_dim)\n",
    "        create_graph: Whether to create computation graph for higher-order derivatives\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Gradient tensor of shape (batch_size, input_dim)\n",
    "    \"\"\"\n",
    "    grad = torch.autograd.grad(\n",
    "        outputs=u,\n",
    "        inputs=x,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=create_graph,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    return grad\n",
    "\n",
    "def compute_laplacian_2d(u, x, create_graph=True):\n",
    "    \"\"\"Compute 2D Laplacian of u with respect to x.\n",
    "    \n",
    "    Args:\n",
    "        u: Output tensor of shape (batch_size, 1)\n",
    "        x: Input tensor of shape (batch_size, 2) where x[:, 0] is x-coordinate and x[:, 1] is y-coordinate\n",
    "        create_graph: Whether to create computation graph\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Laplacian tensor of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    # First derivatives\n",
    "    grad = compute_gradient(u, x, create_graph=create_graph)\n",
    "    u_x = grad[:, 0:1]\n",
    "    u_y = grad[:, 1:2]\n",
    "    \n",
    "    # Second derivatives\n",
    "    u_xx = compute_gradient(u_x, x, create_graph=create_graph)[:, 0:1]\n",
    "    u_yy = compute_gradient(u_y, x, create_graph=create_graph)[:, 1:2]\n",
    "    \n",
    "    # Laplacian\n",
    "    laplacian = u_xx + u_yy\n",
    "    return laplacian\n",
    "\n",
    "def compute_time_derivative(u, x, create_graph=True):\n",
    "    \"\"\"Compute time derivative for time-dependent problems.\n",
    "    \n",
    "    Args:\n",
    "        u: Output tensor of shape (batch_size, 1)\n",
    "        x: Input tensor of shape (batch_size, 2) where x[:, -1] is the time coordinate\n",
    "        create_graph: Whether to create computation graph\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Time derivative tensor of shape (batch_size, 1)\n",
    "    \"\"\"\n",
    "    grad = compute_gradient(u, x, create_graph=create_graph)\n",
    "    u_t = grad[:, -1:]\n",
    "    return u_t\n",
    "\n",
    "# Example: Test gradient computation\n",
    "x_test = torch.randn(10, 2, device=device, requires_grad=True)\n",
    "u_test = model(x_test)\n",
    "grad_test = compute_gradient(u_test, x_test)\n",
    "print(f\"Input shape: {x_test.shape}\")\n",
    "print(f\"Output shape: {u_test.shape}\")\n",
    "print(f\"Gradient shape: {grad_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions\n",
    "\n",
    "Define various loss components for different types of PDEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNLoss:\n",
    "    \"\"\"Container for PINN loss functions\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_weights=None):\n",
    "        self.model = model\n",
    "        self.loss_weights = loss_weights or {'pde': 1.0, 'bc': 1.0, 'ic': 1.0, 'data': 1.0}\n",
    "    \n",
    "    def poisson_2d_loss(self, domain_points, boundary_points, source_function, boundary_function):\n",
    "        \"\"\"Loss function for 2D Poisson equation: ∇²u = f(x,y)\n",
    "        \n",
    "        Args:\n",
    "            domain_points: Interior collocation points\n",
    "            boundary_points: Boundary points\n",
    "            source_function: Source term function f(x,y)\n",
    "            boundary_function: Boundary condition function g(x,y)\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing individual loss components and total loss\n",
    "        \"\"\"\n",
    "        domain_points.requires_grad_(True)\n",
    "        \n",
    "        # PDE residual loss\n",
    "        u_domain = self.model(domain_points)\n",
    "        laplacian = compute_laplacian_2d(u_domain, domain_points)\n",
    "        source = source_function(domain_points)\n",
    "        pde_residual = laplacian - source\n",
    "        loss_pde = torch.mean(pde_residual**2)\n",
    "        \n",
    "        # Boundary condition loss\n",
    "        u_boundary = self.model(boundary_points)\n",
    "        bc_target = boundary_function(boundary_points)\n",
    "        loss_bc = torch.mean((u_boundary - bc_target)**2)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.loss_weights['pde'] * loss_pde + \n",
    "                     self.loss_weights['bc'] * loss_bc)\n",
    "        \n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'pde': loss_pde,\n",
    "            'bc': loss_bc\n",
    "        }\n",
    "    \n",
    "    def heat_equation_loss(self, points_dict, initial_function, boundary_function, alpha=1.0):\n",
    "        \"\"\"Loss function for heat equation: ∂u/∂t = α∇²u\n",
    "        \n",
    "        Args:\n",
    "            points_dict: Dictionary with 'domain', 'boundary', 'initial' points\n",
    "            initial_function: Initial condition function u(x,0)\n",
    "            boundary_function: Boundary condition function\n",
    "            alpha: Thermal diffusivity\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing individual loss components and total loss\n",
    "        \"\"\"\n",
    "        domain_points = points_dict['domain']\n",
    "        boundary_points = points_dict['boundary']\n",
    "        initial_points = points_dict['initial']\n",
    "        \n",
    "        domain_points.requires_grad_(True)\n",
    "        \n",
    "        # PDE residual loss\n",
    "        u_domain = self.model(domain_points)\n",
    "        u_t = compute_time_derivative(u_domain, domain_points)\n",
    "        \n",
    "        # For 1D heat equation, only need second derivative w.r.t. x\n",
    "        grad = compute_gradient(u_domain, domain_points)\n",
    "        u_x = grad[:, 0:1]\n",
    "        u_xx = compute_gradient(u_x, domain_points)[:, 0:1]\n",
    "        \n",
    "        pde_residual = u_t - alpha * u_xx\n",
    "        loss_pde = torch.mean(pde_residual**2)\n",
    "        \n",
    "        # Boundary condition loss\n",
    "        u_boundary = self.model(boundary_points)\n",
    "        bc_target = boundary_function(boundary_points)\n",
    "        loss_bc = torch.mean((u_boundary - bc_target)**2)\n",
    "        \n",
    "        # Initial condition loss\n",
    "        u_initial = self.model(initial_points)\n",
    "        ic_target = initial_function(initial_points)\n",
    "        loss_ic = torch.mean((u_initial - ic_target)**2)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (self.loss_weights['pde'] * loss_pde + \n",
    "                     self.loss_weights['bc'] * loss_bc + \n",
    "                     self.loss_weights['ic'] * loss_ic)\n",
    "        \n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'pde': loss_pde,\n",
    "            'bc': loss_bc,\n",
    "            'ic': loss_ic\n",
    "        }\n",
    "    \n",
    "    def data_loss(self, data_points, data_values):\n",
    "        \"\"\"Data fitting loss for inverse problems\n",
    "        \n",
    "        Args:\n",
    "            data_points: Observation points\n",
    "            data_values: Observation values\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Data fitting loss\n",
    "        \"\"\"\n",
    "        u_pred = self.model(data_points)\n",
    "        return torch.mean((u_pred - data_values)**2)\n",
    "\n",
    "# Example loss function setup\n",
    "loss_computer = PINNLoss(model, loss_weights={'pde': 1.0, 'bc': 10.0, 'ic': 10.0})\n",
    "print(\"Loss computer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Utilities\n",
    "\n",
    "Training functions with support for different optimizers and strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINNTrainer:\n",
    "    \"\"\"PINN training utilities\"\"\"\n",
    "    \n",
    "    def __init__(self, model, loss_computer):\n",
    "        self.model = model\n",
    "        self.loss_computer = loss_computer\n",
    "        self.loss_history = []\n",
    "    \n",
    "    def train_adam(self, loss_function, epochs=10000, lr=1e-3, print_every=1000):\n",
    "        \"\"\"Train with Adam optimizer\n",
    "        \n",
    "        Args:\n",
    "            loss_function: Function that returns loss dictionary\n",
    "            epochs: Number of training epochs\n",
    "            lr: Learning rate\n",
    "            print_every: Print frequency\n",
    "        \n",
    "        Returns:\n",
    "            list: Loss history\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        print(f\"Starting Adam training for {epochs} epochs...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss_dict = loss_function()\n",
    "            total_loss = loss_dict['total']\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Record loss\n",
    "            self.loss_history.append({\n",
    "                'epoch': epoch,\n",
    "                'optimizer': 'Adam',\n",
    "                **{k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}\n",
    "            })\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                print(f\"Epoch {epoch+1:5d}/{epochs} | \"\n",
    "                      f\"Loss: {total_loss.item():.6e} | \"\n",
    "                      f\"Time: {elapsed_time:.1f}s\")\n",
    "                start_time = time.time()\n",
    "        \n",
    "        return self.loss_history\n",
    "    \n",
    "    def train_lbfgs(self, loss_function, max_iter=1000):\n",
    "        \"\"\"Train with L-BFGS optimizer\n",
    "        \n",
    "        Args:\n",
    "            loss_function: Function that returns loss dictionary\n",
    "            max_iter: Maximum number of iterations\n",
    "        \n",
    "        Returns:\n",
    "            list: Loss history\n",
    "        \"\"\"\n",
    "        print(f\"Starting L-BFGS training for up to {max_iter} iterations...\")\n",
    "        \n",
    "        # Convert model parameters to numpy for scipy\n",
    "        def closure():\n",
    "            loss_dict = loss_function()\n",
    "            total_loss = loss_dict['total']\n",
    "            \n",
    "            # Record loss\n",
    "            current_epoch = len([h for h in self.loss_history if h['optimizer'] == 'L-BFGS'])\n",
    "            self.loss_history.append({\n",
    "                'epoch': current_epoch,\n",
    "                'optimizer': 'L-BFGS',\n",
    "                **{k: v.item() if torch.is_tensor(v) else v for k, v in loss_dict.items()}\n",
    "            })\n",
    "            \n",
    "            return total_loss\n",
    "        \n",
    "        # Use PyTorch's L-BFGS optimizer\n",
    "        optimizer = optim.LBFGS(\n",
    "            self.model.parameters(), \n",
    "            max_iter=max_iter,\n",
    "            tolerance_grad=1e-7,\n",
    "            tolerance_change=1e-9,\n",
    "            history_size=100\n",
    "        )\n",
    "        \n",
    "        def step_closure():\n",
    "            optimizer.zero_grad()\n",
    "            loss = closure()\n",
    "            loss.backward()\n",
    "            return loss\n",
    "        \n",
    "        optimizer.step(step_closure)\n",
    "        \n",
    "        return self.loss_history\n",
    "    \n",
    "    def train_two_stage(self, loss_function, adam_epochs=10000, adam_lr=1e-3, \n",
    "                       lbfgs_max_iter=1000, print_every=1000):\n",
    "        \"\"\"Two-stage training: Adam followed by L-BFGS\n",
    "        \n",
    "        Args:\n",
    "            loss_function: Function that returns loss dictionary\n",
    "            adam_epochs: Number of Adam epochs\n",
    "            adam_lr: Adam learning rate\n",
    "            lbfgs_max_iter: L-BFGS max iterations\n",
    "            print_every: Print frequency for Adam\n",
    "        \n",
    "        Returns:\n",
    "            list: Complete loss history\n",
    "        \"\"\"\n",
    "        print(\"=== Two-Stage Training ===\")\n",
    "        print(\"Stage 1: Adam optimization\")\n",
    "        self.train_adam(loss_function, adam_epochs, adam_lr, print_every)\n",
    "        \n",
    "        print(\"\\nStage 2: L-BFGS fine-tuning\")\n",
    "        self.train_lbfgs(loss_function, lbfgs_max_iter)\n",
    "        \n",
    "        return self.loss_history\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"Plot training loss history\"\"\"\n",
    "        if not self.loss_history:\n",
    "            print(\"No training history available\")\n",
    "            return\n",
    "        \n",
    "        adam_history = [h for h in self.loss_history if h['optimizer'] == 'Adam']\n",
    "        lbfgs_history = [h for h in self.loss_history if h['optimizer'] == 'L-BFGS']\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if adam_history:\n",
    "            adam_epochs = [h['epoch'] for h in adam_history]\n",
    "            adam_losses = [h['total'] for h in adam_history]\n",
    "            plt.plot(adam_epochs, adam_losses, 'b-', label='Adam', linewidth=2)\n",
    "        \n",
    "        if lbfgs_history:\n",
    "            lbfgs_start = len(adam_history) if adam_history else 0\n",
    "            lbfgs_epochs = [lbfgs_start + h['epoch'] for h in lbfgs_history]\n",
    "            lbfgs_losses = [h['total'] for h in lbfgs_history]\n",
    "            plt.plot(lbfgs_epochs, lbfgs_losses, 'r-', label='L-BFGS', linewidth=2)\n",
    "        \n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Total Loss')\n",
    "        plt.title('PINN Training Loss History')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Print final losses\n",
    "        if self.loss_history:\n",
    "            final_loss = self.loss_history[-1]\n",
    "            print(f\"\\nFinal Loss: {final_loss['total']:.6e}\")\n",
    "            if 'pde' in final_loss:\n",
    "                print(f\"PDE Loss: {final_loss['pde']:.6e}\")\n",
    "            if 'bc' in final_loss:\n",
    "                print(f\"BC Loss: {final_loss['bc']:.6e}\")\n",
    "            if 'ic' in final_loss:\n",
    "                print(f\"IC Loss: {final_loss['ic']:.6e}\")\n",
    "\n",
    "# Example trainer setup\n",
    "trainer = PINNTrainer(model, loss_computer)\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example: 2D Poisson Equation\n",
    "\n",
    "Complete example solving the 2D Poisson equation: ∇²u = f(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem setup: 2D Poisson equation\n",
    "# PDE: ∇²u = -2π²sin(πx)sin(πy)\n",
    "# Domain: [-1,1] × [-1,1]\n",
    "# BC: u = 0 on boundary\n",
    "# Analytical solution: u(x,y) = sin(πx)sin(πy)\n",
    "\n",
    "# Define problem parameters\n",
    "bounds = [[-1, 1], [-1, 1]]\n",
    "num_domain = 2000\n",
    "num_boundary = 200\n",
    "\n",
    "# Sample points\n",
    "domain_points = sample_domain_2d(bounds, num_domain, device)\n",
    "boundary_points = sample_boundary_2d(bounds, num_boundary//4, device)\n",
    "\n",
    "# Define source function and boundary conditions\n",
    "def source_function(points):\n",
    "    \"\"\"Source term: f(x,y) = -2π²sin(πx)sin(πy)\"\"\"\n",
    "    x, y = points[:, 0:1], points[:, 1:2]\n",
    "    return -2 * np.pi**2 * torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "def boundary_function(points):\n",
    "    \"\"\"Boundary condition: u = 0\"\"\"\n",
    "    return torch.zeros((points.shape[0], 1), device=device)\n",
    "\n",
    "def analytical_solution(points):\n",
    "    \"\"\"Analytical solution: u(x,y) = sin(πx)sin(πy)\"\"\"\n",
    "    x, y = points[:, 0:1], points[:, 1:2]\n",
    "    return torch.sin(np.pi * x) * torch.sin(np.pi * y)\n",
    "\n",
    "# Create model and trainer\n",
    "poisson_model = PINN(input_dim=2, hidden_dim=50, output_dim=1, num_hidden_layers=3).to(device)\n",
    "poisson_loss_computer = PINNLoss(poisson_model, loss_weights={'pde': 1.0, 'bc': 10.0})\n",
    "poisson_trainer = PINNTrainer(poisson_model, poisson_loss_computer)\n",
    "\n",
    "# Define loss function\n",
    "def poisson_loss():\n",
    "    return poisson_loss_computer.poisson_2d_loss(\n",
    "        domain_points, boundary_points, source_function, boundary_function\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "print(\"Training 2D Poisson equation solver...\")\n",
    "history = poisson_trainer.train_two_stage(\n",
    "    poisson_loss, \n",
    "    adam_epochs=5000, \n",
    "    adam_lr=1e-3, \n",
    "    lbfgs_max_iter=500,\n",
    "    print_every=1000\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "poisson_trainer.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization and Validation\n",
    "\n",
    "Functions to visualize results and compute errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_2d_solution(model, bounds, analytical_solution=None, resolution=100):\n",
    "    \"\"\"Visualize 2D solution with comparison to analytical solution\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PINN model\n",
    "        bounds: Domain bounds [[x_min, x_max], [y_min, y_max]]\n",
    "        analytical_solution: Analytical solution function (optional)\n",
    "        resolution: Grid resolution for visualization\n",
    "    \"\"\"\n",
    "    # Create visualization grid\n",
    "    x = torch.linspace(bounds[0][0], bounds[0][1], resolution)\n",
    "    y = torch.linspace(bounds[1][0], bounds[1][1], resolution)\n",
    "    X, Y = torch.meshgrid(x, y, indexing='ij')\n",
    "    \n",
    "    # Flatten for model prediction\n",
    "    xy_test = torch.stack([X.flatten(), Y.flatten()], dim=1).to(device)\n",
    "    \n",
    "    # Predict solution\n",
    "    with torch.no_grad():\n",
    "        u_pred = model(xy_test).cpu()\n",
    "    u_pred_grid = u_pred.reshape(resolution, resolution)\n",
    "    \n",
    "    # Setup plot\n",
    "    if analytical_solution is not None:\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), subplot_kw={'projection': '3d'})\n",
    "        \n",
    "        # Analytical solution\n",
    "        u_analytical = analytical_solution(xy_test).cpu()\n",
    "        u_analytical_grid = u_analytical.reshape(resolution, resolution)\n",
    "        \n",
    "        # Plot analytical solution\n",
    "        X_np, Y_np = X.numpy(), Y.numpy()\n",
    "        surf1 = axes[0].plot_surface(X_np, Y_np, u_analytical_grid.numpy(), cmap='viridis')\n",
    "        axes[0].set_title('Analytical Solution')\n",
    "        axes[0].set_xlabel('x')\n",
    "        axes[0].set_ylabel('y')\n",
    "        \n",
    "        # Plot PINN solution\n",
    "        surf2 = axes[1].plot_surface(X_np, Y_np, u_pred_grid.numpy(), cmap='viridis')\n",
    "        axes[1].set_title('PINN Solution')\n",
    "        axes[1].set_xlabel('x')\n",
    "        axes[1].set_ylabel('y')\n",
    "        \n",
    "        # Plot error\n",
    "        error = torch.abs(u_pred_grid - u_analytical_grid)\n",
    "        surf3 = axes[2].plot_surface(X_np, Y_np, error.numpy(), cmap='hot')\n",
    "        axes[2].set_title('Absolute Error')\n",
    "        axes[2].set_xlabel('x')\n",
    "        axes[2].set_ylabel('y')\n",
    "        \n",
    "        # Compute metrics\n",
    "        l2_error = torch.sqrt(torch.mean((u_pred_grid - u_analytical_grid)**2))\n",
    "        l2_relative = l2_error / torch.sqrt(torch.mean(u_analytical_grid**2))\n",
    "        max_error = torch.max(torch.abs(u_pred_grid - u_analytical_grid))\n",
    "        \n",
    "        print(f\"L2 Error: {l2_error:.6e}\")\n",
    "        print(f\"L2 Relative Error: {l2_relative:.6e}\")\n",
    "        print(f\"Max Absolute Error: {max_error:.6e}\")\n",
    "        \n",
    "    else:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        \n",
    "        X_np, Y_np = X.numpy(), Y.numpy()\n",
    "        surf = ax.plot_surface(X_np, Y_np, u_pred_grid.numpy(), cmap='viridis')\n",
    "        ax.set_title('PINN Solution')\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        \n",
    "        plt.colorbar(surf)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def compute_metrics(u_pred, u_true):\n",
    "    \"\"\"Compute various error metrics\n",
    "    \n",
    "    Args:\n",
    "        u_pred: Predicted solution\n",
    "        u_true: True solution\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of error metrics\n",
    "    \"\"\"\n",
    "    l2_error = torch.sqrt(torch.mean((u_pred - u_true)**2))\n",
    "    l2_relative = l2_error / torch.sqrt(torch.mean(u_true**2))\n",
    "    l_inf_error = torch.max(torch.abs(u_pred - u_true))\n",
    "    \n",
    "    return {\n",
    "        'l2_absolute': l2_error.item(),\n",
    "        'l2_relative': l2_relative.item(),\n",
    "        'l_inf': l_inf_error.item()\n",
    "    }\n",
    "\n",
    "# Visualize the trained Poisson model\n",
    "print(\"Visualizing Poisson equation results...\")\n",
    "visualize_2d_solution(poisson_model, bounds, analytical_solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advanced Features\n",
    "\n",
    "Additional utilities for more complex PINN implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPINNFeatures:\n",
    "    \"\"\"Advanced PINN features and techniques\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def adaptive_weights(loss_dict, alpha=0.5, max_weight=1000.0):\n",
    "        \"\"\"Adaptive loss weighting using GradNorm-like approach\n",
    "        \n",
    "        Args:\n",
    "            loss_dict: Dictionary of individual losses\n",
    "            alpha: Weighting parameter\n",
    "            max_weight: Maximum weight value\n",
    "        \n",
    "        Returns:\n",
    "            dict: Updated loss weights\n",
    "        \"\"\"\n",
    "        # Simple adaptive weighting based on loss magnitudes\n",
    "        losses = {k: v for k, v in loss_dict.items() if k != 'total'}\n",
    "        \n",
    "        if len(losses) < 2:\n",
    "            return {k: 1.0 for k in losses.keys()}\n",
    "        \n",
    "        # Compute relative loss rates\n",
    "        loss_values = torch.tensor(list(losses.values()))\n",
    "        avg_loss = torch.mean(loss_values)\n",
    "        \n",
    "        weights = {}\n",
    "        for k, v in losses.items():\n",
    "            # Higher weight for larger losses\n",
    "            weight = torch.clamp(avg_loss / (v + 1e-10), 0.1, max_weight)\n",
    "            weights[k] = weight.item()\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def causal_weighting(points, current_time, time_window=0.1):\n",
    "        \"\"\"Causal weighting for time-dependent problems\n",
    "        \n",
    "        Args:\n",
    "            points: Time-space points tensor\n",
    "            current_time: Current time step in training\n",
    "            time_window: Width of the causal window\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Causal weights\n",
    "        \"\"\"\n",
    "        t = points[:, -1]  # Assume last dimension is time\n",
    "        weights = torch.exp(-torch.clamp((t - current_time) / time_window, 0, 10))\n",
    "        return weights.unsqueeze(-1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_enhanced_loss(model, data_points, data_values, grad_points, grad_values):\n",
    "        \"\"\"Gradient-enhanced PINN (gPINN) loss\n",
    "        \n",
    "        Args:\n",
    "            model: PINN model\n",
    "            data_points: Points where solution values are known\n",
    "            data_values: Known solution values\n",
    "            grad_points: Points where gradient values are known\n",
    "            grad_values: Known gradient values\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Combined data and gradient loss\n",
    "        \"\"\"\n",
    "        # Standard data loss\n",
    "        u_pred = model(data_points)\n",
    "        data_loss = torch.mean((u_pred - data_values)**2)\n",
    "        \n",
    "        # Gradient loss\n",
    "        grad_points.requires_grad_(True)\n",
    "        u_grad_pred = model(grad_points)\n",
    "        grad_pred = compute_gradient(u_grad_pred, grad_points)\n",
    "        grad_loss = torch.mean((grad_pred - grad_values)**2)\n",
    "        \n",
    "        return data_loss + grad_loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def fourier_feature_embedding(x, num_frequencies=10, sigma=1.0):\n",
    "        \"\"\"Fourier feature embedding for better high-frequency learning\n",
    "        \n",
    "        Args:\n",
    "            x: Input coordinates\n",
    "            num_frequencies: Number of frequency components\n",
    "            sigma: Standard deviation for random frequencies\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Fourier features\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        dtype = x.dtype\n",
    "        \n",
    "        # Random frequency matrix\n",
    "        B = torch.randn(x.shape[-1], num_frequencies, device=device, dtype=dtype) * sigma\n",
    "        \n",
    "        # Compute projections\n",
    "        x_proj = 2 * np.pi * x @ B\n",
    "        \n",
    "        # Sine and cosine features\n",
    "        features = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Example: Using Fourier features\n",
    "class FourierPINN(nn.Module):\n",
    "    \"\"\"PINN with Fourier feature embedding\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_frequencies=10, sigma=1.0):\n",
    "        super(FourierPINN, self).__init__()\n",
    "        \n",
    "        self.num_frequencies = num_frequencies\n",
    "        self.sigma = sigma\n",
    "        \n",
    "        # Fourier embedding increases input dimension\n",
    "        fourier_dim = 2 * num_frequencies\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(fourier_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply Fourier feature embedding\n",
    "        fourier_features = AdvancedPINNFeatures.fourier_feature_embedding(\n",
    "            x, self.num_frequencies, self.sigma\n",
    "        )\n",
    "        return self.network(fourier_features)\n",
    "\n",
    "print(\"Advanced PINN features loaded\")\n",
    "\n",
    "# Example: Create a Fourier PINN\n",
    "fourier_model = FourierPINN(input_dim=2, hidden_dim=64, output_dim=1, num_frequencies=10).to(device)\n",
    "print(f\"Fourier PINN created with {sum(p.numel() for p in fourier_model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Template Usage Guide\n",
    "\n",
    "This template provides a complete framework for implementing PINNs in PyTorch. Here's how to use it for different types of problems:\n",
    "\n",
    "### For Time-Dependent PDEs (e.g., Heat Equation):\n",
    "1. Use `sample_time_domain()` to generate points\n",
    "2. Use `heat_equation_loss()` method from `PINNLoss`\n",
    "3. Include initial conditions in your loss function\n",
    "\n",
    "### For Elliptic PDEs (e.g., Poisson Equation):\n",
    "1. Use `sample_domain_2d()` and `sample_boundary_2d()`\n",
    "2. Use `poisson_2d_loss()` method from `PINNLoss`\n",
    "3. Focus on boundary conditions\n",
    "\n",
    "### For Inverse Problems:\n",
    "1. Add unknown parameters as model attributes\n",
    "2. Use `data_loss()` method for observation data\n",
    "3. Include parameter regularization if needed\n",
    "\n",
    "### For Advanced Techniques:\n",
    "1. Use `FourierPINN` for high-frequency problems\n",
    "2. Apply `gradient_enhanced_loss()` when derivative data is available\n",
    "3. Use `adaptive_weights()` for multi-objective optimization\n",
    "\n",
    "### Training Strategy:\n",
    "1. Start with `train_adam()` for initial exploration\n",
    "2. Use `train_two_stage()` for best accuracy\n",
    "3. Monitor loss components individually\n",
    "4. Adjust loss weights based on problem requirements\n",
    "\n",
    "This template replaces the DeepXDE dependency while providing equivalent functionality with more flexibility and control over the implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}