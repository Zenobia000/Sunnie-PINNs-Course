{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Physics-Informed Neural Network (VPINN) - PyTorch Implementation\n",
    "\n",
    "This notebook implements a Variational Physics-Informed Neural Network (VPINN) using PyTorch to solve a 1D Poisson equation using the weak formulation.\n",
    "\n",
    "## Problem Definition:\n",
    "- **PDE**: $-u_{xx}(x) = \\pi^2 \\sin(\\pi x)$\n",
    "- **Domain**: $x \\in [-1, 1]$\n",
    "- **Boundary Conditions**: $u(-1) = 0$, $u(1) = 0$\n",
    "- **Analytical Solution**: $u(x) = \\sin(\\pi x)$\n",
    "\n",
    "## Key VPINN Concept:\n",
    "Instead of directly enforcing the PDE residual, VPINN uses the **weak form** of the PDE:\n",
    "$$\\int_{\\Omega} u_x v_x \\, dx = \\int_{\\Omega} f v \\, dx$$\n",
    "where $v$ are test functions that satisfy the boundary conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List, Callable\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Problem Setup and Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain boundaries\n",
    "x_min, x_max = -1.0, 1.0\n",
    "\n",
    "def source_function(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Source function f(x) = π² sin(πx).\"\"\"\n",
    "    return torch.pi**2 * torch.sin(torch.pi * x)\n",
    "\n",
    "def analytical_solution(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Analytical solution: u(x) = sin(πx).\"\"\"\n",
    "    return torch.sin(torch.pi * x)\n",
    "\n",
    "def boundary_condition(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Boundary condition: u(-1) = u(1) = 0.\"\"\"\n",
    "    return torch.zeros_like(x[:, 0:1])\n",
    "\n",
    "print(f\"Domain: [{x_min}, {x_max}]\")\n",
    "print(f\"PDE: -u_xx = π² sin(πx)\")\n",
    "print(f\"Analytical solution: u(x) = sin(πx)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Functions for Variational Formulation\n",
    "\n",
    "Test functions $v(x)$ must satisfy the boundary conditions (i.e., $v(-1) = v(1) = 0$) to properly enforce the variational formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestFunctions:\n",
    "    \"\"\"Collection of test functions and their derivatives for VPINN.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def v_0(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Test function v₀(x) = 1 - x².\"\"\"\n",
    "        return 1 - x**2\n",
    "    \n",
    "    @staticmethod\n",
    "    def v_0_x(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Derivative of v₀: v₀'(x) = -2x.\"\"\"\n",
    "        return -2 * x\n",
    "    \n",
    "    @staticmethod\n",
    "    def v_1(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Test function v₁(x) = x(1 - x²).\"\"\"\n",
    "        return x * (1 - x**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def v_1_x(x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Derivative of v₁: v₁'(x) = 1 - 3x².\"\"\"\n",
    "        return 1 - 3 * x**2\n",
    "    \n",
    "    @classmethod\n",
    "    def get_test_functions(cls) -> List[Tuple[Callable, Callable]]:\n",
    "        \"\"\"Return list of (test_function, derivative) pairs.\"\"\"\n",
    "        return [(cls.v_0, cls.v_0_x), (cls.v_1, cls.v_1_x)]\n",
    "\n",
    "# Visualize test functions\n",
    "x_plot = torch.linspace(x_min, x_max, 100).unsqueeze(1)\n",
    "test_funcs = TestFunctions.get_test_functions()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, (v, v_x) in enumerate(test_funcs):\n",
    "    plt.plot(x_plot.numpy(), v(x_plot).numpy(), label=f'$v_{i}(x)$', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('v(x)')\n",
    "plt.title('Test Functions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, (v, v_x) in enumerate(test_funcs):\n",
    "    plt.plot(x_plot.numpy(), v_x(x_plot).numpy(), label=f\"$v'_{i}(x)$\", linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"v'(x)\")\n",
    "plt.title('Test Function Derivatives')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of test functions: {len(test_funcs)}\")\n",
    "print(\"Test functions satisfy boundary conditions: v(-1) = v(1) = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_domain_points(n_points: int) -> torch.Tensor:\n",
    "    \"\"\"Generate random points in the domain for Monte Carlo integration.\"\"\"\n",
    "    x = torch.rand(n_points, 1) * (x_max - x_min) + x_min\n",
    "    return x.to(device)\n",
    "\n",
    "def generate_boundary_points(n_points: int) -> torch.Tensor:\n",
    "    \"\"\"Generate points on the boundary.\"\"\"\n",
    "    # Half points at x = -1, half at x = 1\n",
    "    n_left = n_points // 2\n",
    "    n_right = n_points - n_left\n",
    "    \n",
    "    x_left = torch.full((n_left, 1), x_min)\n",
    "    x_right = torch.full((n_right, 1), x_max)\n",
    "    \n",
    "    boundary_points = torch.cat([x_left, x_right], dim=0)\n",
    "    return boundary_points.to(device)\n",
    "\n",
    "def generate_test_points(n_points: int) -> torch.Tensor:\n",
    "    \"\"\"Generate test points for evaluation.\"\"\"\n",
    "    x = torch.linspace(x_min, x_max, n_points).unsqueeze(1)\n",
    "    return x.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPINN(nn.Module):\n",
    "    \"\"\"Variational Physics-Informed Neural Network.\"\"\"\n",
    "    \n",
    "    def __init__(self, layers: List[int] = [1, 20, 20, 20, 1]):\n",
    "        super(VPINN, self).__init__()\n",
    "        \n",
    "        # Neural network layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(layers[i], layers[i + 1]))\n",
    "        \n",
    "        # Initialize weights using Xavier normal\n",
    "        self.init_weights()\n",
    "        \n",
    "        print(f\"VPINN initialized with {len(layers)} layers: {layers}\")\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize network weights using Xavier normal initialization.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the network.\"\"\"\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = torch.tanh(layer(x))\n",
    "        # Final layer without activation\n",
    "        x = self.layers[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variational Loss Functions\n",
    "\n",
    "The key innovation of VPINN is using the **weak form** of the PDE instead of the strong form. For our Poisson equation:\n",
    "\n",
    "**Strong form**: $-u_{xx} = f$\n",
    "\n",
    "**Weak form**: $\\int_{\\Omega} u_x v_x \\, dx = \\int_{\\Omega} f v \\, dx$ for all test functions $v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_derivative(u: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute first derivative du/dx using automatic differentiation.\"\"\"\n",
    "    u_x = torch.autograd.grad(u, x, torch.ones_like(u), \n",
    "                             create_graph=True, retain_graph=True)[0]\n",
    "    return u_x\n",
    "\n",
    "def variational_residual(model: VPINN, x: torch.Tensor, test_functions: List[Tuple[Callable, Callable]]) -> torch.Tensor:\n",
    "    \"\"\"Compute variational residual using weak form of PDE.\"\"\"\n",
    "    x.requires_grad_(True)\n",
    "    u = model(x)\n",
    "    u_x = compute_derivative(u, x)\n",
    "    \n",
    "    # Source function values\n",
    "    f_val = source_function(x)\n",
    "    \n",
    "    residuals = []\n",
    "    for v, v_x in test_functions:\n",
    "        # Test function and its derivative values\n",
    "        v_val = v(x)\n",
    "        v_x_val = v_x(x)\n",
    "        \n",
    "        # Weak form: ∫(u_x * v_x)dx - ∫(f * v)dx = 0\n",
    "        # Monte Carlo approximation: mean over batch points\n",
    "        integrand = u_x * v_x_val - f_val * v_val\n",
    "        integral_residual = torch.mean(integrand)\n",
    "        \n",
    "        residuals.append(integral_residual)\n",
    "    \n",
    "    # Sum of squared residuals from all test functions\n",
    "    total_residual = torch.sum(torch.stack(residuals)**2)\n",
    "    \n",
    "    return total_residual\n",
    "\n",
    "def compute_losses(model: VPINN, domain_points: torch.Tensor, \n",
    "                  boundary_points: torch.Tensor, test_functions: List[Tuple[Callable, Callable]]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute variational and boundary losses.\"\"\"\n",
    "    \n",
    "    # Variational (PDE) loss using weak form\n",
    "    vpde_loss = variational_residual(model, domain_points, test_functions)\n",
    "    \n",
    "    # Boundary condition loss\n",
    "    bc_pred = model(boundary_points)\n",
    "    bc_true = boundary_condition(boundary_points)\n",
    "    bc_loss = torch.mean((bc_pred - bc_true)**2)\n",
    "    \n",
    "    return vpde_loss, bc_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VPINN([1, 20, 20, 20, 1]).to(device)\n",
    "test_functions = TestFunctions.get_test_functions()\n",
    "\n",
    "# Generate training data\n",
    "n_domain = 500  # More points for better Monte Carlo integration\n",
    "n_boundary = 2\n",
    "\n",
    "domain_points = generate_domain_points(n_domain)\n",
    "boundary_points = generate_boundary_points(n_boundary)\n",
    "\n",
    "print(f\"Training data generated:\")\n",
    "print(f\"  Domain points: {domain_points.shape}\")\n",
    "print(f\"  Boundary points: {boundary_points.shape}\")\n",
    "print(f\"  Test functions: {len(test_functions)}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training parameters\n",
    "epochs = 15000\n",
    "log_interval = 1000\n",
    "\n",
    "# Storage for loss history\n",
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"\\nStarting VPINN training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Compute losses\n",
    "    vpde_loss, bc_loss = compute_losses(model, domain_points, boundary_points, test_functions)\n",
    "    total_loss = vpde_loss + bc_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store history\n",
    "    loss_history.append([total_loss.item(), vpde_loss.item(), bc_loss.item()])\n",
    "    \n",
    "    # Logging\n",
    "    if (epoch + 1) % log_interval == 0:\n",
    "        print(f\"Epoch {epoch + 1:5d}/{epochs} | \"\n",
    "              f\"Total: {total_loss.item():.2e} | \"\n",
    "              f\"VPDE: {vpde_loss.item():.2e} | \"\n",
    "              f\"BC: {bc_loss.item():.2e}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "loss_history = np.array(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_history[:, 0], 'b-', label='Total Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Total Loss History')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(loss_history[:, 1], 'r-', label='Variational PDE Loss', linewidth=2)\n",
    "plt.plot(loss_history[:, 2], 'g-', label='Boundary Loss', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Individual Loss Components')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data and evaluate model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_test = generate_test_points(100)\n",
    "    u_pred = model(x_test).cpu().numpy()\n",
    "    u_true = analytical_solution(x_test).cpu().numpy()\n",
    "    x_test_np = x_test.cpu().numpy()\n",
    "\n",
    "# Calculate L2 relative error\n",
    "l2_error = np.linalg.norm(u_true - u_pred) / np.linalg.norm(u_true)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Solution comparison\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(x_test_np, u_true, 'b-', label='Analytical Solution', linewidth=3)\n",
    "plt.plot(x_test_np, u_pred, 'r--', label='VPINN Prediction', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('u(x)')\n",
    "plt.title('VPINN vs. Analytical Solution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Error plot\n",
    "plt.subplot(1, 3, 2)\n",
    "error = np.abs(u_true - u_pred)\n",
    "plt.plot(x_test_np, error, 'g-', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('|Error|')\n",
    "plt.title(f'Absolute Error\\nL2 Relative Error: {l2_error:.2e}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Source function and solution\n",
    "plt.subplot(1, 3, 3)\n",
    "f_vals = source_function(x_test).cpu().numpy()\n",
    "plt.plot(x_test_np, f_vals, 'm-', label='Source f(x)', linewidth=2)\n",
    "plt.plot(x_test_np, u_true, 'b-', label='Solution u(x)', linewidth=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Source Function and Solution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Variational Form Verification\n",
    "\n",
    "Let's verify that our solution satisfies the weak form by checking the residuals for each test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify weak form residuals\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Use a dense set of points for accurate integration\n",
    "    x_verify = generate_test_points(1000)\n",
    "    x_verify.requires_grad_(True)\n",
    "    \n",
    "    u = model(x_verify)\n",
    "    u_x = compute_derivative(u, x_verify)\n",
    "    f_val = source_function(x_verify)\n",
    "    \n",
    "    print(\"Verification of Weak Form Residuals:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    for i, (v, v_x) in enumerate(test_functions):\n",
    "        v_val = v(x_verify)\n",
    "        v_x_val = v_x(x_verify)\n",
    "        \n",
    "        # Compute weak form residual: ∫(u_x * v_x - f * v)dx\n",
    "        integrand = u_x * v_x_val - f_val * v_val\n",
    "        residual = torch.mean(integrand).item()\n",
    "        \n",
    "        print(f\"Test function v_{i}: Residual = {residual:.2e}\")\n",
    "    \n",
    "    print(\"\\nSmall residuals indicate good satisfaction of the weak form!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: Strong Form vs. Weak Form\n",
    "\n",
    "Let's compare the residuals when evaluated using both strong and weak formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strong form vs weak form residuals\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    x_comp = generate_test_points(100)\n",
    "    x_comp.requires_grad_(True)\n",
    "    \n",
    "    u = model(x_comp)\n",
    "    u_x = compute_derivative(u, x_comp)\n",
    "    u_xx = torch.autograd.grad(u_x, x_comp, torch.ones_like(u_x), \n",
    "                              create_graph=True, retain_graph=True)[0]\n",
    "    \n",
    "    # Strong form residual: -u_xx - f\n",
    "    f_val = source_function(x_comp)\n",
    "    strong_residual = -u_xx - f_val\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Strong form residual\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x_comp.cpu().numpy(), strong_residual.cpu().numpy(), 'r-', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Strong Form Residual: -u_xx - f')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weak form verification for first test function\n",
    "    plt.subplot(1, 2, 2)\n",
    "    v, v_x = test_functions[0]\n",
    "    v_val = v(x_comp)\n",
    "    v_x_val = v_x(x_comp)\n",
    "    weak_integrand = u_x * v_x_val - f_val * v_val\n",
    "    \n",
    "    plt.plot(x_comp.cpu().numpy(), weak_integrand.cpu().numpy(), 'b-', linewidth=2)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('Integrand')\n",
    "    plt.title('Weak Form Integrand: u_x*v_x - f*v')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Strong form RMS residual: {torch.sqrt(torch.mean(strong_residual**2)).item():.2e}\")\n",
    "    print(f\"Weak form integral residual: {torch.mean(weak_integrand).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final analysis\n",
    "final_total_loss = loss_history[-1, 0]\n",
    "final_vpde_loss = loss_history[-1, 1]\n",
    "final_bc_loss = loss_history[-1, 2]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VARIATIONAL PINN (VPINN) TRAINING SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Problem: 1D Poisson Equation with Variational Formulation\")\n",
    "print(f\"Network Architecture: {[1, 20, 20, 20, 1]}\")\n",
    "print(f\"Training Epochs: {epochs:,}\")\n",
    "print(f\"Domain Points: {n_domain:,} (for Monte Carlo integration)\")\n",
    "print(f\"Test Functions: {len(test_functions)}\")\n",
    "print()\n",
    "print(\"FINAL RESULTS:\")\n",
    "print(f\"  Total Loss: {final_total_loss:.4e}\")\n",
    "print(f\"  Variational PDE Loss: {final_vpde_loss:.4e}\")\n",
    "print(f\"  Boundary Loss: {final_bc_loss:.4e}\")\n",
    "print(f\"  L2 Relative Error: {l2_error:.4e}\")\n",
    "print()\n",
    "print(\"KEY FEATURES OF VPINN:\")\n",
    "print(\"✓ Uses weak formulation instead of strong form PDE residual\")\n",
    "print(\"✓ Employs test functions that satisfy boundary conditions\")\n",
    "print(\"✓ Monte Carlo integration for variational integrals\")\n",
    "print(\"✓ Better numerical stability for certain PDE types\")\n",
    "print(\"✓ Natural incorporation of boundary conditions in weak form\")\n",
    "print(\"✓ Can handle problems where strong form is difficult to enforce\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}