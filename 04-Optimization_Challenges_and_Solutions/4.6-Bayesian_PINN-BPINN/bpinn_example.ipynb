{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian PINN (BPINN) for Uncertainty Quantification\n",
        "\n",
        "### 1. Introduction: Beyond Point Predictions\n",
        "\n",
        "Standard (deterministic) PINNs provide a single point prediction for the solution of a PDE. However, in many scientific and engineering applications, it's crucial to know how confident the model is in its predictions. This is where **Uncertainty Quantification (UQ)** comes in.\n",
        "\n",
        "The **Bayesian Physics-Informed Neural Network (BPINN)** is a framework that incorporates Bayesian inference into the PINN model. Instead of learning a single set of weights, a BPINN learns a *distribution* over the network weights. This allows it to output not just a mean prediction but also a measure of uncertainty (typically the variance or standard deviation).\n",
        "\n",
        "This uncertainty can arise from two main sources:\n",
        "-   **Aleatoric uncertainty:** Inherent randomness or noise in the data.\n",
        "-   **Epistemic uncertainty:** Uncertainty due to the model's lack of knowledge, often higher in regions with sparse training data.\n",
        "\n",
        "### 2. Demonstration: 1D Poisson Equation with `PFNN`\n",
        "\n",
        "This notebook demonstrates a practical way to implement a BPINN using DeepXDE's built-in **Probabilistic Feed-forward Neural Network (`dde.nn.PFNN`)**. This network architecture is designed to output two values for each input point:\n",
        "1.  The **mean** ($\\mu$) of the predicted solution.\n",
        "2.  The **log-variance** ($\\log(\\sigma^2)$) of the predicted solution.\n",
        "\n",
        "We will solve a simple 1D Poisson equation and use the `PFNN`'s outputs to visualize the mean prediction along with its confidence interval (uncertainty bounds).\n",
        "\n",
        "**Problem Definition:**\n",
        "-   **PDE:** $-u_{xx}(x) = \\pi^2 \\sin(\\pi x)$\n",
        "-   **Domain:** $x \\in [-1, 1]$\n",
        "-   **Boundary Conditions:** $u(-1) = 0, u(1) = 0$\n",
        "-   **Analytical Solution:** $u(x) = \\sin(\\pi x)$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import deepxde as dde\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Set backend to PyTorch for this example if needed\n",
        "# dde.config.set_default_backend(\"pytorch\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Define the PDE, Geometry, and BCs\n",
        "\n",
        "First, we define the PDE residual function. A key aspect of this BPINN implementation is that the physics-informed loss is only applied to the **mean** of the network's probabilistic output. The network's second output (the variance) is constrained by the data likelihood during training, not directly by the PDE.\n",
        "\n",
        "The boundary conditions are also applied only to the mean prediction. We specify `component=0` in the `DirichletBC` to ensure this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Define the PDE and analytical solution\n",
        "def pde_func(x, y):\n",
        "    \"\"\"Standard strong-form PDE residual for -u_xx = f(x)\"\"\"\n",
        "    # y has two components: y[:, 0:1] is the mean, y[:, 1:2] is the log(variance)\n",
        "    u_mean = y[:, 0:1]\n",
        "    \n",
        "    # We only apply the PDE to the mean of the prediction\n",
        "    du_xx = dde.grad.hessian(u_mean, x, i=0, j=0)\n",
        "    \n",
        "    # Use torch functions for GPU compatibility and consistency with the backend\n",
        "    source_term = torch.pi**2 * torch.sin(torch.pi * x)\n",
        "    \n",
        "    return -du_xx - source_term\n",
        "\n",
        "def analytical_solution(x):\n",
        "    # This is used for testing, so it operates on NumPy arrays\n",
        "    return np.sin(np.pi * x)\n",
        "\n",
        "# 2. Define Geometry and Boundary Conditions\n",
        "geom = dde.geometry.Interval(-1, 1)\n",
        "\n",
        "# The boundary condition is applied to the mean of the prediction (component=0)\n",
        "bc = dde.DirichletBC(geom, lambda x: 0, lambda _, on_boundary: on_boundary, component=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Define and Train the BPINN Model\n",
        "\n",
        "Now, we define the BPINN model using `dde.nn.PFNN`. The network will have 1 input neuron (`x`) and 2 output neurons (mean and log-variance).\n",
        "\n",
        "When we compile the model, we don't need to specify a custom loss. DeepXDE automatically recognizes that `PFNN` is a probabilistic network and uses the appropriate loss function, which is the **negative log-likelihood**. This loss naturally encourages the network to produce a smaller variance where the mean prediction is accurate and a larger variance where it is less certain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Compiling model...\n",
            "'compile' took 0.000307 s\n",
            "\n",
            "Training model...\n",
            "\n",
            "Step      Train loss              Test loss               Test metric\n",
            "0         [4.82e+01, 5.54e-02]    [5.01e+01, 5.54e-02]    []  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1000      [2.90e-04, 4.16e-07]    [3.13e-04, 4.16e-07]    []  \n",
            "2000      [1.69e-04, 2.54e-07]    [1.84e-04, 2.54e-07]    []  \n",
            "3000      [1.21e-04, 2.85e-07]    [1.42e-04, 2.85e-07]    []  \n",
            "4000      [6.78e-05, 1.79e-07]    [9.18e-05, 1.79e-07]    []  \n",
            "5000      [4.13e-05, 8.99e-09]    [6.83e-05, 8.99e-09]    []  \n",
            "6000      [2.91e-05, 8.33e-08]    [5.09e-05, 8.33e-08]    []  \n"
          ]
        }
      ],
      "source": [
        "# 3. Define the BPINN Model using a Probabilistic FNN\n",
        "# It has 2 outputs: mean and log(variance)\n",
        "net = dde.nn.PFNN([1] + [20] * 3 + [2], \"tanh\", \"Glorot normal\")\n",
        "\n",
        "# 4. Define the Data object\n",
        "data = dde.data.PDE(\n",
        "    geom,\n",
        "    pde_func,\n",
        "    bc,\n",
        "    num_domain=20,\n",
        "    num_boundary=2,\n",
        "    solution=analytical_solution,\n",
        "    num_test=100,\n",
        ")\n",
        "\n",
        "# 5. Compile and Train the Model\n",
        "model = dde.Model(data, net)\n",
        "# The default loss for a probabilistic network in DeepXDE is the negative log-likelihood\n",
        "model.compile(\"adam\", lr=1e-3)\n",
        "losshistory, train_state = model.train(iterations=20000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Uncertainty Quantification and Visualization\n",
        "\n",
        "After training, the model can predict both the mean and the log-variance for any given input point `x`. To visualize the uncertainty, we perform the following steps:\n",
        "\n",
        "1.  Predict on a dense set of test points to get the `mean` and `log_variance` outputs.\n",
        "2.  Calculate the standard deviation ($\\sigma$) from the log-variance using the formula: $\\sigma = \\sqrt{e^{\\log(\\sigma^2)}}$.\n",
        "3.  Plot the analytical solution and the BPINN's mean prediction.\n",
        "4.  Use `plt.fill_between` to draw a shaded region around the mean prediction. This region typically represents a confidence interval, such as $\\mu \\pm 2\\sigma$ (which corresponds to a ~95% confidence interval for a Gaussian distribution).\n",
        "\n",
        "This plot provides a powerful visual summary: not only how well the model's average prediction matches the true solution, but also in which areas the model is more or less confident.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'deepxde' has no attribute 'plot'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6. Plot the loss history first\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdde\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[38;5;241m.\u001b[39mplot_loss_history(losshistory)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss History for BPINN (1D Poisson)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
            "\u001b[1;31mAttributeError\u001b[0m: module 'deepxde' has no attribute 'plot'"
          ]
        }
      ],
      "source": [
        "# 6. Plotting and Results\n",
        "# Manually plot the loss history\n",
        "loss_train = np.sum(np.array(losshistory.loss_train), axis=1)\n",
        "loss_test = np.sum(np.array(losshistory.loss_test), axis=1)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(losshistory.steps, loss_train, 'b-', label='Train Loss')\n",
        "plt.plot(losshistory.steps, loss_test, 'r--', label='Test Loss')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.yscale('log')\n",
        "plt.title('Loss History for BPINN (1D Poisson)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 7. Uncertainty Quantification and Visualization\n",
        "X_test = geom.uniform_points(200, True)\n",
        "y_pred = model.predict(X_test)\n",
        "y_true = analytical_solution(X_test)\n",
        "\n",
        "mean_pred = y_pred[:, 0]\n",
        "log_var_pred = y_pred[:, 1]\n",
        "std_dev_pred = np.sqrt(np.exp(log_var_pred))\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.plot(X_test, y_true, 'b-', label='Analytical Solution', linewidth=2)\n",
        "plt.plot(X_test, mean_pred, 'r--', label='BPINN Mean Prediction', linewidth=2)\n",
        "\n",
        "# Plot the uncertainty region (e.g., 2 standard deviations)\n",
        "plt.fill_between(\n",
        "    X_test.flatten(), \n",
        "    mean_pred - 2 * std_dev_pred, \n",
        "    mean_pred + 2 * std_dev_pred, \n",
        "    color='orange', \n",
        "    alpha=0.3, \n",
        "    label='Uncertainty (2 std. dev.)'\n",
        ")\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('u(x)')\n",
        "plt.title('BPINN Prediction with Uncertainty Quantification')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Report L2 relative error on the mean prediction\n",
        "l2_error = dde.metrics.l2_relative_error(y_true, mean_pred.reshape(-1, 1))\n",
        "print(f\"L2 relative error on mean: {l2_error:.4e}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
