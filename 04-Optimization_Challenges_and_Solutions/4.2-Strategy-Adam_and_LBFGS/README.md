# 4.2 - 實踐策略：Adam 與 L-BFGS 的兩階段優化

## 1. 引言：為何標準優化器還不夠？

在前面的章節中，我們主要使用 Adam 優化器來訓練 PINN 模型。Adam 因其快速收斂和對複雜損失景觀的良好探索能力而成為深度學習中的首選。然而，在追求高精度科學計算解的場景下，單純使用 Adam 可能會遇到瓶頸。

**核心挑戰**：
- **收斂精度限制**：Adam 是一種基於一階梯度的隨機優化算法。當接近損失函數的最小值點時，由於其固有的動量和隨機性，往往會在最優解附近**震盪**，難以精確收斂到最小值。
- **效率問題**：為了達到極高的精度，可能需要非常長的訓練時間和極小的學習率，這在計算上是不划算的。

本章節將介紹一種在 PINN 領域非常經典且高效的**兩階段優化策略**，即結合 **Adam** 和 **L-BFGS** 優化器，以實現快速探索和高精度收斂的雙重目標。

---

## 2. 兩種優化器的特性分析

### a. Adam (Adaptive Moment Estimation) - 探索者

- **類型**：一階隨機梯度下降 (SGD) 的變體。
- **工作原理**：結合了動量 (Momentum) 和 RMSProp 的思想，為每個參數計算自適應的學習率。
- **優點**：
    - **全局探索能力強**：能夠快速在複雜的、非凸的損失景觀中找到一個包含潛在最優解的「寬闊山谷」。
    - **訓練初期收斂快**：非常適合在訓練開始時，損失值較大的情況下快速降低損失。
- **缺點**：
    - **收斂精度不足**：在最優解附近容易震盪，難以達到機器精度級別的收斂。

### b. L-BFGS (Limited-memory Broyden–Fletcher–Goldfarb–Shanno) - 精修者

- **類型**：擬牛頓法 (Quasi-Newton Method)，屬於二階優化方法。
- **工作原理**：通過儲存過去的梯度資訊來近似損失函數的海森矩陣 (Hessian Matrix) 的逆，從而計算出更優的下降方向和步長。
- **優點**：
    - **收斂精度極高**：一旦進入最優解的鄰近區域（通常是凸區域），L-BFGS 能夠利用二階曲率資訊，非常高效地收斂到該區域的精確最小值。
- **缺點**：
    - **對初始值敏感**：如果初始點離最優解太遠，可能會收斂失敗或收斂到較差的局部最小值。
    - **計算成本較高**：每一步的計算量比 Adam 大，不適合在整個訓練過程中全程使用。

---

## 3. 兩階段優化策略：「先粗後精」

結合兩者的優點，我們設計出如下的兩階段訓練流程：

1.  **階段一：使用 Adam 進行探索性訓練**
    - **目標**：利用 Adam 的快速收斂特性，在成千上萬次的迭代中，將神經網路的權重引導到一個「有希望」的區域，使損失函數的值降低到一個合理的範圍內（例如 `1e-3` 到 `1e-4`）。
    - **比喻**：Adam 就像一個探險家，快速地在地圖上找到一個藏有寶藏的山谷。

2.  **階段二：切換到 L-BFGS 進行精細微調**
    - **目標**：在 Adam 找到的「好」解的基礎上，利用 L-BFGS 的高精度收斂特性，對權重進行精細調整，將損失函數進一步推向其精確的最小值。
    - **比喻**：L-BFGS 則像一個考古學家，在探險家找到的山谷中，用精密儀器準確地定位並挖掘出寶藏。

在接下來的 Python 腳本 `optimization_strategy_example.py` 中，我們將通過一個簡單的 ODE 範例，量化比較「僅使用 Adam」和「Adam + L-BFGS」兩種策略下的最終解的精度差異，以展示此方法的強大之處。
