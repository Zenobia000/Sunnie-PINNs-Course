{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2: Two-Stage Optimization Strategy - PyTorch Implementation\n",
    "\n",
    "This notebook demonstrates the powerful two-stage optimization strategy (Adam + L-BFGS) for training Physics-Informed Neural Networks using pure PyTorch. We'll solve a simple ODE and compare different optimization approaches.\n",
    "\n",
    "### Problem Definition:\n",
    "- **ODE**: $\\frac{dy}{dx} + y = 0$\n",
    "- **Initial Condition**: $y(0) = 1$\n",
    "- **Domain**: $x \\in [0, 5]$\n",
    "- **Analytical Solution**: $y(x) = e^{-x}$\n",
    "\n",
    "### Optimization Strategies to Compare:\n",
    "1. **Adam Only**: Standard adaptive optimizer\n",
    "2. **L-BFGS Only**: Quasi-Newton method\n",
    "3. **Two-Stage**: Adam (exploration) + L-BFGS (refinement)\n",
    "4. **Advanced**: Adaptive techniques with learning rate scheduling\n",
    "\n",
    "### Key Insights:\n",
    "- Adam excels at initial exploration of complex loss landscapes\n",
    "- L-BFGS provides superior final convergence precision\n",
    "- Two-stage approach combines the best of both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from scipy.optimize import minimize\n",
    "from copy import deepcopy\n",
    "\n",
    "# Set device and style\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network and Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    \"\"\"Physics-Informed Neural Network for ODE solving\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim=32, num_layers=3):\n",
    "        super(PINN, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(1, hidden_dim))  # Input: x\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        layers.append(nn.Tanh())\n",
    "        layers.append(nn.Linear(hidden_dim, 1))  # Output: y\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier/Glorot normal initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    def get_weights_vector(self):\n",
    "        \"\"\"Get all parameters as a single vector (for L-BFGS)\"\"\"\n",
    "        return torch.cat([p.view(-1) for p in self.parameters()])\n",
    "    \n",
    "    def set_weights_vector(self, weights):\n",
    "        \"\"\"Set all parameters from a single vector (for L-BFGS)\"\"\"\n",
    "        start = 0\n",
    "        for param in self.parameters():\n",
    "            end = start + param.numel()\n",
    "            param.data = weights[start:end].view(param.shape)\n",
    "            start = end\n",
    "    \n",
    "    def clone_weights(self):\n",
    "        \"\"\"Create a copy of current weights\"\"\"\n",
    "        return {name: param.clone() for name, param in self.named_parameters()}\n",
    "    \n",
    "    def load_weights(self, weight_dict):\n",
    "        \"\"\"Load weights from a dictionary\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            param.data = weight_dict[name].clone()\n",
    "\n",
    "def analytical_solution(x):\n",
    "    \"\"\"Analytical solution: y(x) = exp(-x)\"\"\"\n",
    "    return torch.exp(-x)\n",
    "\n",
    "def compute_loss(model, x_domain, x0, y0):\n",
    "    \"\"\"Compute the composite PINN loss\"\"\"\n",
    "    # Enable gradients for automatic differentiation\n",
    "    x_domain.requires_grad_(True)\n",
    "    \n",
    "    # ODE residual loss: dy/dx + y = 0\n",
    "    y_pred = model(x_domain)\n",
    "    dy_dx = torch.autograd.grad(\n",
    "        y_pred, x_domain,\n",
    "        grad_outputs=torch.ones_like(y_pred),\n",
    "        create_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    ode_residual = dy_dx + y_pred\n",
    "    loss_ode = torch.mean(ode_residual**2)\n",
    "    \n",
    "    # Initial condition loss: y(0) = 1\n",
    "    y0_pred = model(x0)\n",
    "    loss_ic = torch.mean((y0_pred - y0)**2)\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = loss_ode + loss_ic\n",
    "    \n",
    "    return total_loss, loss_ode, loss_ic\n",
    "\n",
    "# Problem setup\n",
    "x_domain = torch.linspace(0, 5, 100, requires_grad=True).view(-1, 1).to(device)\n",
    "x0 = torch.tensor([[0.0]], device=device)\n",
    "y0 = torch.tensor([[1.0]], device=device)\n",
    "\n",
    "print(f\"Domain points: {x_domain.shape}\")\n",
    "print(f\"Initial condition: y({x0.item():.1f}) = {y0.item():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimization Strategy Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationTracker:\n",
    "    \"\"\"Track optimization progress and metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, model, x_test=None):\n",
    "        self.model = model\n",
    "        self.x_test = x_test if x_test is not None else torch.linspace(0, 5, 200).view(-1, 1).to(device)\n",
    "        self.history = []\n",
    "        self.start_time = None\n",
    "        self.phase_times = []\n",
    "    \n",
    "    def start_tracking(self, phase_name):\n",
    "        \"\"\"Start tracking a new optimization phase\"\"\"\n",
    "        self.start_time = time.time()\n",
    "        self.current_phase = phase_name\n",
    "    \n",
    "    def record_step(self, epoch, total_loss, ode_loss, ic_loss, optimizer_name=None):\n",
    "        \"\"\"Record a single optimization step\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Compute L2 error against analytical solution\n",
    "            y_pred = self.model(self.x_test)\n",
    "            y_true = analytical_solution(self.x_test)\n",
    "            l2_error = torch.sqrt(torch.mean((y_pred - y_true)**2)).item()\n",
    "            l2_relative = l2_error / torch.sqrt(torch.mean(y_true**2)).item()\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time if self.start_time else 0\n",
    "        \n",
    "        self.history.append({\n",
    "            'epoch': epoch,\n",
    "            'phase': getattr(self, 'current_phase', 'unknown'),\n",
    "            'optimizer': optimizer_name or getattr(self, 'current_phase', 'unknown'),\n",
    "            'total_loss': total_loss,\n",
    "            'ode_loss': ode_loss,\n",
    "            'ic_loss': ic_loss,\n",
    "            'l2_error': l2_error,\n",
    "            'l2_relative': l2_relative,\n",
    "            'time': elapsed_time\n",
    "        })\n",
    "    \n",
    "    def end_phase(self):\n",
    "        \"\"\"End current optimization phase\"\"\"\n",
    "        if self.start_time:\n",
    "            phase_time = time.time() - self.start_time\n",
    "            self.phase_times.append(phase_time)\n",
    "            self.start_time = None\n",
    "            return phase_time\n",
    "        return 0\n",
    "    \n",
    "    def get_final_metrics(self):\n",
    "        \"\"\"Get final performance metrics\"\"\"\n",
    "        if not self.history:\n",
    "            return {}\n",
    "        \n",
    "        final_record = self.history[-1]\n",
    "        return {\n",
    "            'final_loss': final_record['total_loss'],\n",
    "            'final_l2_error': final_record['l2_error'],\n",
    "            'final_l2_relative': final_record['l2_relative'],\n",
    "            'total_epochs': len(self.history),\n",
    "            'total_time': sum(self.phase_times)\n",
    "        }\n",
    "\n",
    "class AdamOptimizer:\n",
    "    \"\"\"Adam optimizer implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr, betas=betas, eps=eps)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.8, patience=1000, verbose=False\n",
    "        )\n",
    "    \n",
    "    def train(self, x_domain, x0, y0, epochs=10000, print_every=1000, tracker=None):\n",
    "        \"\"\"Train with Adam optimizer\"\"\"\n",
    "        if tracker:\n",
    "            tracker.start_tracking('Adam')\n",
    "        \n",
    "        print(f\"Starting Adam training for {epochs} epochs...\")\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss, ode_loss, ic_loss = compute_loss(self.model, x_domain, x0, y0)\n",
    "            total_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step(total_loss)\n",
    "            \n",
    "            if tracker:\n",
    "                tracker.record_step(epoch, total_loss.item(), ode_loss.item(), \n",
    "                                  ic_loss.item(), 'Adam')\n",
    "            \n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                lr = self.optimizer.param_groups[0]['lr']\n",
    "                print(f\"  Epoch {epoch+1:5d} | Loss: {total_loss.item():.2e} | \"\n",
    "                      f\"ODE: {ode_loss.item():.2e} | IC: {ic_loss.item():.2e} | LR: {lr:.1e}\")\n",
    "        \n",
    "        if tracker:\n",
    "            phase_time = tracker.end_phase()\n",
    "            print(f\"Adam training completed in {phase_time:.1f}s\")\n",
    "        \n",
    "        return tracker.history if tracker else []\n",
    "\n",
    "class LBFGSOptimizer:\n",
    "    \"\"\"L-BFGS optimizer implementation using PyTorch's L-BFGS\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1, max_iter=20, tolerance_grad=1e-7, tolerance_change=1e-9):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.LBFGS(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            max_iter=max_iter,\n",
    "            tolerance_grad=tolerance_grad,\n",
    "            tolerance_change=tolerance_change,\n",
    "            history_size=100\n",
    "        )\n",
    "        self.current_epoch = 0\n",
    "        self.tracker = None\n",
    "    \n",
    "    def train(self, x_domain, x0, y0, max_iterations=1000, print_every=50, tracker=None):\n",
    "        \"\"\"Train with L-BFGS optimizer\"\"\"\n",
    "        self.tracker = tracker\n",
    "        self.current_epoch = 0\n",
    "        \n",
    "        if tracker:\n",
    "            tracker.start_tracking('L-BFGS')\n",
    "        \n",
    "        print(f\"Starting L-BFGS training for up to {max_iterations} iterations...\")\n",
    "        \n",
    "        def closure():\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss, ode_loss, ic_loss = compute_loss(self.model, x_domain, x0, y0)\n",
    "            total_loss.backward()\n",
    "            \n",
    "            if self.tracker:\n",
    "                self.tracker.record_step(self.current_epoch, total_loss.item(), \n",
    "                                       ode_loss.item(), ic_loss.item(), 'L-BFGS')\n",
    "            \n",
    "            if self.current_epoch % print_every == 0:\n",
    "                print(f\"  Iter {self.current_epoch:4d} | Loss: {total_loss.item():.2e} | \"\n",
    "                      f\"ODE: {ode_loss.item():.2e} | IC: {ic_loss.item():.2e}\")\n",
    "            \n",
    "            self.current_epoch += 1\n",
    "            return total_loss\n",
    "        \n",
    "        # Run L-BFGS optimization\n",
    "        for i in range(max_iterations // 20):  # L-BFGS does multiple iterations per step\n",
    "            self.optimizer.step(closure)\n",
    "            \n",
    "            # Check for convergence\n",
    "            with torch.no_grad():\n",
    "                current_loss, _, _ = compute_loss(self.model, x_domain, x0, y0)\n",
    "                if current_loss.item() < 1e-10:\n",
    "                    print(f\"  Early convergence achieved at iteration {self.current_epoch}\")\n",
    "                    break\n",
    "        \n",
    "        if tracker:\n",
    "            phase_time = tracker.end_phase()\n",
    "            print(f\"L-BFGS training completed in {phase_time:.1f}s\")\n",
    "        \n",
    "        return tracker.history if tracker else []\n",
    "\n",
    "class TwoStageOptimizer:\n",
    "    \"\"\"Two-stage optimization: Adam followed by L-BFGS\"\"\"\n",
    "    \n",
    "    def __init__(self, model, adam_lr=1e-3, lbfgs_lr=1):\n",
    "        self.model = model\n",
    "        self.adam_optimizer = AdamOptimizer(model, lr=adam_lr)\n",
    "        self.lbfgs_optimizer = LBFGSOptimizer(model, lr=lbfgs_lr)\n",
    "    \n",
    "    def train(self, x_domain, x0, y0, adam_epochs=10000, lbfgs_iterations=500, \n",
    "              print_every=1000, tracker=None):\n",
    "        \"\"\"Two-stage training\"\"\"\n",
    "        print(\"=== TWO-STAGE OPTIMIZATION ===\")\n",
    "        print(\"Stage 1: Adam optimization (exploration phase)\")\n",
    "        \n",
    "        # Stage 1: Adam\n",
    "        self.adam_optimizer.train(x_domain, x0, y0, adam_epochs, print_every, tracker)\n",
    "        \n",
    "        print(\"\\nStage 2: L-BFGS optimization (refinement phase)\")\n",
    "        \n",
    "        # Stage 2: L-BFGS\n",
    "        self.lbfgs_optimizer.train(x_domain, x0, y0, lbfgs_iterations, \n",
    "                                 max(print_every//20, 10), tracker)\n",
    "        \n",
    "        print(\"Two-stage optimization completed!\")\n",
    "        \n",
    "        return tracker.history if tracker else []\n",
    "\n",
    "print(\"Optimization classes initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Study: Different Optimization Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_optimization_comparison():\n",
    "    \"\"\"Run comprehensive comparison of optimization strategies\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test configurations\n",
    "    strategies = {\n",
    "        'Adam Only': {\n",
    "            'class': AdamOptimizer,\n",
    "            'params': {'lr': 1e-3},\n",
    "            'train_params': {'epochs': 15000, 'print_every': 2000}\n",
    "        },\n",
    "        'L-BFGS Only': {\n",
    "            'class': LBFGSOptimizer,\n",
    "            'params': {'lr': 1, 'max_iter': 20},\n",
    "            'train_params': {'max_iterations': 500, 'print_every': 50}\n",
    "        },\n",
    "        'Two-Stage': {\n",
    "            'class': TwoStageOptimizer,\n",
    "            'params': {'adam_lr': 1e-3, 'lbfgs_lr': 1},\n",
    "            'train_params': {'adam_epochs': 10000, 'lbfgs_iterations': 300, 'print_every': 2000}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Starting Optimization Strategy Comparison...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for strategy_name, config in strategies.items():\n",
    "        print(f\"\\n\\nTesting Strategy: {strategy_name}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create fresh model for each strategy\n",
    "        model = PINN(hidden_dim=32, num_layers=3).to(device)\n",
    "        \n",
    "        # Initialize tracker\n",
    "        tracker = OptimizationTracker(model)\n",
    "        \n",
    "        # Create optimizer\n",
    "        optimizer = config['class'](model, **config['params'])\n",
    "        \n",
    "        # Train\n",
    "        start_time = time.time()\n",
    "        history = optimizer.train(x_domain, x0, y0, **config['train_params'], tracker=tracker)\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Collect results\n",
    "        final_metrics = tracker.get_final_metrics()\n",
    "        final_metrics['strategy'] = strategy_name\n",
    "        final_metrics['history'] = history\n",
    "        final_metrics['model_state'] = model.clone_weights()\n",
    "        \n",
    "        results[strategy_name] = final_metrics\n",
    "        \n",
    "        print(f\"\\nFinal Results for {strategy_name}:\")\n",
    "        print(f\"  Final Loss: {final_metrics['final_loss']:.2e}\")\n",
    "        print(f\"  L2 Error: {final_metrics['final_l2_error']:.2e}\")\n",
    "        print(f\"  L2 Relative: {final_metrics['final_l2_relative']:.2e}\")\n",
    "        print(f\"  Total Time: {final_metrics['total_time']:.1f}s\")\n",
    "        print(f\"  Total Epochs/Iterations: {final_metrics['total_epochs']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "comparison_results = run_optimization_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detailed Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_optimization_comparison(results):\n",
    "    \"\"\"Create comprehensive visualization of optimization comparison\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Colors for different strategies\n",
    "    colors = {'Adam Only': 'blue', 'L-BFGS Only': 'red', 'Two-Stage': 'green'}\n",
    "    \n",
    "    # 1. Loss convergence\n",
    "    ax = axes[0, 0]\n",
    "    for strategy, result in results.items():\n",
    "        history = result['history']\n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        losses = [h['total_loss'] for h in history]\n",
    "        \n",
    "        # Handle two-stage plotting\n",
    "        if strategy == 'Two-Stage':\n",
    "            # Split into Adam and L-BFGS phases\n",
    "            adam_history = [h for h in history if h['optimizer'] == 'Adam']\n",
    "            lbfgs_history = [h for h in history if h['optimizer'] == 'L-BFGS']\n",
    "            \n",
    "            if adam_history:\n",
    "                adam_epochs = [h['epoch'] for h in adam_history]\n",
    "                adam_losses = [h['total_loss'] for h in adam_history]\n",
    "                ax.plot(adam_epochs, adam_losses, color=colors[strategy], linewidth=2, alpha=0.7)\n",
    "            \n",
    "            if lbfgs_history:\n",
    "                # Continue epoch counting from Adam phase\n",
    "                offset = len(adam_history) if adam_history else 0\n",
    "                lbfgs_epochs = [h['epoch'] + offset for h in lbfgs_history]\n",
    "                lbfgs_losses = [h['total_loss'] for h in lbfgs_history]\n",
    "                ax.plot(lbfgs_epochs, lbfgs_losses, color=colors[strategy], linewidth=3, \n",
    "                       linestyle='--', label=f'{strategy} (L-BFGS phase)')\n",
    "                \n",
    "                # Mark transition point\n",
    "                if adam_history:\n",
    "                    transition_x = len(adam_history)\n",
    "                    transition_y = adam_losses[-1]\n",
    "                    ax.axvline(x=transition_x, color=colors[strategy], linestyle=':', alpha=0.5)\n",
    "                    ax.plot(transition_x, transition_y, 'o', color=colors[strategy], \n",
    "                           markersize=8, label=f'{strategy} (transition)')\n",
    "        else:\n",
    "            ax.plot(epochs, losses, color=colors[strategy], linewidth=2, label=strategy)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Total Loss')\n",
    "    ax.set_title('Loss Convergence Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. L2 Error evolution\n",
    "    ax = axes[0, 1]\n",
    "    for strategy, result in results.items():\n",
    "        history = result['history']\n",
    "        epochs = [h['epoch'] for h in history]\n",
    "        l2_errors = [h['l2_error'] for h in history]\n",
    "        ax.plot(epochs, l2_errors, color=colors[strategy], linewidth=2, label=strategy)\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('L2 Error')\n",
    "    ax.set_title('L2 Error Evolution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Final performance comparison\n",
    "    ax = axes[0, 2]\n",
    "    strategies = list(results.keys())\n",
    "    final_losses = [results[s]['final_loss'] for s in strategies]\n",
    "    final_l2_errors = [results[s]['final_l2_error'] for s in strategies]\n",
    "    \n",
    "    x_pos = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x_pos - width/2, np.log10(final_losses), width, \n",
    "           label='log₁₀(Final Loss)', alpha=0.7, color='lightblue')\n",
    "    ax.bar(x_pos + width/2, np.log10(final_l2_errors), width, \n",
    "           label='log₁₀(L2 Error)', alpha=0.7, color='lightcoral')\n",
    "    \n",
    "    ax.set_xlabel('Strategy')\n",
    "    ax.set_ylabel('log₁₀(Error)')\n",
    "    ax.set_title('Final Performance Comparison')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(strategies, rotation=45)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Solution accuracy visualization\n",
    "    ax = axes[1, 0]\n",
    "    x_test = torch.linspace(0, 5, 200).view(-1, 1).to(device)\n",
    "    y_true = analytical_solution(x_test).cpu().numpy()\n",
    "    \n",
    "    ax.plot(x_test.cpu().numpy(), y_true, 'k-', linewidth=3, label='Analytical', alpha=0.8)\n",
    "    \n",
    "    for strategy, result in results.items():\n",
    "        # Create a temporary model to load weights\n",
    "        temp_model = PINN(hidden_dim=32, num_layers=3).to(device)\n",
    "        temp_model.load_weights(result['model_state'])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = temp_model(x_test).cpu().numpy()\n",
    "        \n",
    "        ax.plot(x_test.cpu().numpy(), y_pred, color=colors[strategy], \n",
    "               linewidth=2, linestyle='--', label=f'{strategy}', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y(x)')\n",
    "    ax.set_title('Solution Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Training time comparison\n",
    "    ax = axes[1, 1]\n",
    "    training_times = [results[s]['total_time'] for s in strategies]\n",
    "    bars = ax.bar(strategies, training_times, color=[colors[s] for s in strategies], alpha=0.7)\n",
    "    \n",
    "    # Add time labels on bars\n",
    "    for bar, time_val in zip(bars, training_times):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                f'{time_val:.1f}s', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    ax.set_ylabel('Training Time (seconds)')\n",
    "    ax.set_title('Training Time Comparison')\n",
    "    ax.set_xticklabels(strategies, rotation=45)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Efficiency analysis (accuracy vs time)\n",
    "    ax = axes[1, 2]\n",
    "    for strategy, result in results.items():\n",
    "        x_val = result['total_time']\n",
    "        y_val = -np.log10(result['final_l2_error'])  # Higher is better (lower error)\n",
    "        ax.scatter(x_val, y_val, color=colors[strategy], s=200, alpha=0.7, \n",
    "                  edgecolors='black', linewidth=2, label=strategy)\n",
    "        \n",
    "        # Add strategy labels\n",
    "        ax.annotate(strategy, (x_val, y_val), xytext=(5, 5), \n",
    "                   textcoords='offset points', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Training Time (seconds)')\n",
    "    ax.set_ylabel('-log₁₀(L2 Error) [Higher = Better]')\n",
    "    ax.set_title('Efficiency Analysis: Accuracy vs Time')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create comprehensive visualization\n",
    "plot_optimization_comparison(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detailed Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimization_performance(results):\n",
    "    \"\"\"Detailed analysis of optimization performance\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"                   OPTIMIZATION STRATEGY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create summary table\n",
    "    print(f\"\\n{'Strategy':<15} {'Final Loss':<12} {'L2 Error':<12} {'L2 Relative':<12} {'Time (s)':<10} {'Epochs':<8}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for strategy, result in results.items():\n",
    "        print(f\"{strategy:<15} {result['final_loss']:<12.2e} \"\n",
    "              f\"{result['final_l2_error']:<12.2e} {result['final_l2_relative']:<12.2e} \"\n",
    "              f\"{result['total_time']:<10.1f} {result['total_epochs']:<8}\")\n",
    "    \n",
    "    # Find best performer in each category\n",
    "    best_accuracy = min(results.items(), key=lambda x: x[1]['final_l2_error'])\n",
    "    best_speed = min(results.items(), key=lambda x: x[1]['total_time'])\n",
    "    best_efficiency = min(results.items(), key=lambda x: x[1]['final_l2_error'] * x[1]['total_time'])\n",
    "    \n",
    "    print(f\"\\n{'PERFORMANCE RANKINGS:':<30}\")\n",
    "    print(f\"{'Best Accuracy:':<20} {best_accuracy[0]} (L2 Error: {best_accuracy[1]['final_l2_error']:.2e})\")\n",
    "    print(f\"{'Fastest Training:':<20} {best_speed[0]} ({best_speed[1]['total_time']:.1f}s)\")\n",
    "    print(f\"{'Best Efficiency:':<20} {best_efficiency[0]} (error×time: {best_efficiency[1]['final_l2_error'] * best_efficiency[1]['total_time']:.2e})\")\n",
    "    \n",
    "    # Convergence analysis\n",
    "    print(f\"\\n{'CONVERGENCE ANALYSIS:':<30}\")\n",
    "    for strategy, result in results.items():\n",
    "        history = result['history']\n",
    "        if len(history) < 10:\n",
    "            continue\n",
    "            \n",
    "        # Find when loss drops below certain thresholds\n",
    "        thresholds = [1e-2, 1e-4, 1e-6]\n",
    "        convergence_epochs = {}\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            for i, h in enumerate(history):\n",
    "                if h['total_loss'] < threshold:\n",
    "                    convergence_epochs[threshold] = i\n",
    "                    break\n",
    "            else:\n",
    "                convergence_epochs[threshold] = None\n",
    "        \n",
    "        print(f\"\\n{strategy}:\")\n",
    "        for threshold, epoch in convergence_epochs.items():\n",
    "            if epoch is not None:\n",
    "                print(f\"  Loss < {threshold:.0e}: epoch {epoch}\")\n",
    "            else:\n",
    "                print(f\"  Loss < {threshold:.0e}: not achieved\")\n",
    "    \n",
    "    # Two-stage analysis\n",
    "    if 'Two-Stage' in results:\n",
    "        print(f\"\\n{'TWO-STAGE DETAILED ANALYSIS:':<30}\")\n",
    "        two_stage_history = results['Two-Stage']['history']\n",
    "        \n",
    "        adam_history = [h for h in two_stage_history if h['optimizer'] == 'Adam']\n",
    "        lbfgs_history = [h for h in two_stage_history if h['optimizer'] == 'L-BFGS']\n",
    "        \n",
    "        if adam_history and lbfgs_history:\n",
    "            adam_final_loss = adam_history[-1]['total_loss']\n",
    "            adam_time = adam_history[-1]['time']\n",
    "            lbfgs_improvement = adam_final_loss / lbfgs_history[-1]['total_loss']\n",
    "            lbfgs_time = lbfgs_history[-1]['time'] - adam_time\n",
    "            \n",
    "            print(f\"  Adam Phase:\")\n",
    "            print(f\"    Final loss: {adam_final_loss:.2e}\")\n",
    "            print(f\"    Time: {adam_time:.1f}s\")\n",
    "            print(f\"    Epochs: {len(adam_history)}\")\n",
    "            \n",
    "            print(f\"  L-BFGS Phase:\")\n",
    "            print(f\"    Final loss: {lbfgs_history[-1]['total_loss']:.2e}\")\n",
    "            print(f\"    Improvement factor: {lbfgs_improvement:.1f}x\")\n",
    "            print(f\"    Time: {lbfgs_time:.1f}s\")\n",
    "            print(f\"    Iterations: {len(lbfgs_history)}\")\n",
    "            \n",
    "            print(f\"  Overall Benefit:\")\n",
    "            print(f\"    L-BFGS achieved {lbfgs_improvement:.1f}x better accuracy\")\n",
    "            print(f\"    in only {lbfgs_time/adam_time*100:.1f}% additional time\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nRECOMMENDATIONS:\")\n",
    "    print(\"\" + \"-\"*20)\n",
    "    \n",
    "    if best_accuracy[0] == 'Two-Stage':\n",
    "        print(\"✓ Two-Stage optimization achieves the highest accuracy\")\n",
    "    \n",
    "    if best_efficiency[0] == 'Two-Stage':\n",
    "        print(\"✓ Two-Stage optimization provides the best accuracy-time tradeoff\")\n",
    "    \n",
    "    adam_only_result = results.get('Adam Only', {})\n",
    "    two_stage_result = results.get('Two-Stage', {})\n",
    "    \n",
    "    if (adam_only_result and two_stage_result and \n",
    "        two_stage_result['final_l2_error'] < adam_only_result['final_l2_error'] * 0.1):\n",
    "        print(\"✓ L-BFGS refinement provides significant accuracy improvement\")\n",
    "    \n",
    "    print(\"\\nBest practices:\")\n",
    "    print(\"  • Use Adam for initial exploration (handles complex loss landscapes)\")\n",
    "    print(\"  • Follow with L-BFGS for high-precision convergence\")\n",
    "    print(\"  • Monitor loss plateaus to determine optimal transition point\")\n",
    "    print(\"  • L-BFGS is most effective when starting from a good Adam solution\")\n",
    "    \n",
    "# Run detailed analysis\n",
    "analyze_optimization_performance(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Two-Stage Strategy with Adaptive Switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveTwoStageOptimizer:\n",
    "    \"\"\"Advanced two-stage optimizer with adaptive switching criteria\"\"\"\n",
    "    \n",
    "    def __init__(self, model, adam_lr=1e-3, lbfgs_lr=1, patience=500, \n",
    "                 min_improvement=1e-6, switch_threshold=1e-4):\n",
    "        self.model = model\n",
    "        self.adam_lr = adam_lr\n",
    "        self.lbfgs_lr = lbfgs_lr\n",
    "        self.patience = patience\n",
    "        self.min_improvement = min_improvement\n",
    "        self.switch_threshold = switch_threshold\n",
    "        \n",
    "    def train(self, x_domain, x0, y0, max_epochs=20000, print_every=1000):\n",
    "        \"\"\"Adaptive two-stage training with intelligent switching\"\"\"\n",
    "        \n",
    "        tracker = OptimizationTracker(self.model)\n",
    "        \n",
    "        print(\"=== ADAPTIVE TWO-STAGE OPTIMIZATION ===\")\n",
    "        print(f\"Switch criteria: loss < {self.switch_threshold:.0e} OR plateau for {self.patience} epochs\")\n",
    "        \n",
    "        # Stage 1: Adam with adaptive switching\n",
    "        print(\"\\nStage 1: Adam optimization with adaptive monitoring...\")\n",
    "        tracker.start_tracking('Adaptive-Adam')\n",
    "        \n",
    "        optimizer_adam = optim.Adam(self.model.parameters(), lr=self.adam_lr)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer_adam, mode='min', factor=0.5, patience=200, verbose=True\n",
    "        )\n",
    "        \n",
    "        loss_history = []\n",
    "        best_loss = float('inf')\n",
    "        stagnation_count = 0\n",
    "        switch_epoch = None\n",
    "        \n",
    "        for epoch in range(max_epochs):\n",
    "            self.model.train()\n",
    "            optimizer_adam.zero_grad()\n",
    "            \n",
    "            total_loss, ode_loss, ic_loss = compute_loss(self.model, x_domain, x0, y0)\n",
    "            total_loss.backward()\n",
    "            optimizer_adam.step()\n",
    "            scheduler.step(total_loss)\n",
    "            \n",
    "            current_loss = total_loss.item()\n",
    "            loss_history.append(current_loss)\n",
    "            \n",
    "            tracker.record_step(epoch, current_loss, ode_loss.item(), \n",
    "                              ic_loss.item(), 'Adaptive-Adam')\n",
    "            \n",
    "            # Check for improvement\n",
    "            if current_loss < best_loss - self.min_improvement:\n",
    "                best_loss = current_loss\n",
    "                stagnation_count = 0\n",
    "            else:\n",
    "                stagnation_count += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                lr = optimizer_adam.param_groups[0]['lr']\n",
    "                print(f\"  Epoch {epoch+1:5d} | Loss: {current_loss:.2e} | \"\n",
    "                      f\"Best: {best_loss:.2e} | Stagnation: {stagnation_count} | LR: {lr:.1e}\")\n",
    "            \n",
    "            # Check switching criteria\n",
    "            should_switch = (\n",
    "                current_loss < self.switch_threshold or  # Low enough loss\n",
    "                stagnation_count >= self.patience        # Plateaued\n",
    "            )\n",
    "            \n",
    "            if should_switch:\n",
    "                switch_epoch = epoch\n",
    "                switch_reason = \"loss threshold\" if current_loss < self.switch_threshold else \"plateau detected\"\n",
    "                print(f\"\\n  → Switching to L-BFGS at epoch {epoch+1} (reason: {switch_reason})\")\n",
    "                print(f\"  → Adam final loss: {current_loss:.2e}\")\n",
    "                break\n",
    "        \n",
    "        adam_time = tracker.end_phase()\n",
    "        \n",
    "        # Stage 2: L-BFGS refinement\n",
    "        if switch_epoch is not None:\n",
    "            print(f\"\\nStage 2: L-BFGS refinement...\")\n",
    "            tracker.start_tracking('Adaptive-LBFGS')\n",
    "            \n",
    "            optimizer_lbfgs = optim.LBFGS(\n",
    "                self.model.parameters(),\n",
    "                lr=self.lbfgs_lr,\n",
    "                max_iter=20,\n",
    "                tolerance_grad=1e-9,\n",
    "                tolerance_change=1e-12,\n",
    "                history_size=100\n",
    "            )\n",
    "            \n",
    "            lbfgs_epoch = 0\n",
    "            \n",
    "            def closure():\n",
    "                nonlocal lbfgs_epoch\n",
    "                optimizer_lbfgs.zero_grad()\n",
    "                total_loss, ode_loss, ic_loss = compute_loss(self.model, x_domain, x0, y0)\n",
    "                total_loss.backward()\n",
    "                \n",
    "                tracker.record_step(switch_epoch + lbfgs_epoch, total_loss.item(), \n",
    "                                  ode_loss.item(), ic_loss.item(), 'Adaptive-LBFGS')\n",
    "                \n",
    "                if lbfgs_epoch % 50 == 0:\n",
    "                    print(f\"  L-BFGS Iter {lbfgs_epoch:3d} | Loss: {total_loss.item():.2e}\")\n",
    "                \n",
    "                lbfgs_epoch += 1\n",
    "                return total_loss\n",
    "            \n",
    "            # Run L-BFGS optimization\n",
    "            for i in range(25):  # Max L-BFGS steps\n",
    "                optimizer_lbfgs.step(closure)\n",
    "                \n",
    "                # Check for convergence\n",
    "                with torch.no_grad():\n",
    "                    current_loss, _, _ = compute_loss(self.model, x_domain, x0, y0)\n",
    "                    if current_loss.item() < 1e-12:\n",
    "                        print(f\"  → L-BFGS converged to machine precision at step {i+1}\")\n",
    "                        break\n",
    "            \n",
    "            lbfgs_time = tracker.end_phase()\n",
    "            \n",
    "            # Final assessment\n",
    "            final_loss, _, _ = compute_loss(self.model, x_domain, x0, y0)\n",
    "            improvement_factor = loss_history[switch_epoch] / final_loss.item()\n",
    "            \n",
    "            print(f\"\\n=== ADAPTIVE OPTIMIZATION RESULTS ===\")\n",
    "            print(f\"Adam Phase: {switch_epoch+1:,} epochs, {adam_time:.1f}s\")\n",
    "            print(f\"L-BFGS Phase: {lbfgs_epoch} iterations, {lbfgs_time:.1f}s\")\n",
    "            print(f\"Final improvement: {improvement_factor:.1f}x better than Adam\")\n",
    "            print(f\"Total time: {adam_time + lbfgs_time:.1f}s\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\nAdam training completed without switching (max epochs reached)\")\n",
    "        \n",
    "        return tracker\n",
    "\n",
    "# Test the adaptive optimizer\n",
    "print(\"Testing Adaptive Two-Stage Optimizer...\")\n",
    "adaptive_model = PINN(hidden_dim=32, num_layers=3).to(device)\n",
    "adaptive_optimizer = AdaptiveTwoStageOptimizer(\n",
    "    adaptive_model, \n",
    "    adam_lr=1e-3, \n",
    "    patience=1000, \n",
    "    switch_threshold=1e-5\n",
    ")\n",
    "\n",
    "adaptive_tracker = adaptive_optimizer.train(x_domain, x0, y0, max_epochs=15000, print_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_solution_quality():\n",
    "    \"\"\"Demonstrate the final solution quality\"\"\"\n",
    "    \n",
    "    # Create test points\n",
    "    x_test = torch.linspace(0, 5, 1000).view(-1, 1).to(device)\n",
    "    y_true = analytical_solution(x_test)\n",
    "    \n",
    "    # Get predictions from adaptive model\n",
    "    with torch.no_grad():\n",
    "        y_pred = adaptive_model(x_test)\n",
    "    \n",
    "    # Calculate comprehensive error metrics\n",
    "    l2_error = torch.sqrt(torch.mean((y_pred - y_true)**2))\n",
    "    l2_relative = l2_error / torch.sqrt(torch.mean(y_true**2))\n",
    "    l_inf_error = torch.max(torch.abs(y_pred - y_true))\n",
    "    \n",
    "    # Point-wise error analysis\n",
    "    pointwise_error = torch.abs(y_pred - y_true)\n",
    "    mean_error = torch.mean(pointwise_error)\n",
    "    std_error = torch.std(pointwise_error)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"           FINAL SOLUTION QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nERROR METRICS:\")\n",
    "    print(f\"  L2 Absolute Error:     {l2_error.item():.2e}\")\n",
    "    print(f\"  L2 Relative Error:     {l2_relative.item():.2e}\")\n",
    "    print(f\"  L∞ (Maximum) Error:    {l_inf_error.item():.2e}\")\n",
    "    print(f\"  Mean Pointwise Error:  {mean_error.item():.2e}\")\n",
    "    print(f\"  Std Pointwise Error:   {std_error.item():.2e}\")\n",
    "    \n",
    "    # Test specific points\n",
    "    test_points = torch.tensor([[0.0], [1.0], [2.0], [3.0], [4.0], [5.0]], device=device)\n",
    "    \n",
    "    print(f\"\\nPOINT-WISE VALIDATION:\")\n",
    "    print(f\"{'x':<6} {'True':<12} {'Predicted':<12} {'Error':<12} {'Rel Error':<10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val in test_points:\n",
    "            y_true_pt = analytical_solution(x_val).item()\n",
    "            y_pred_pt = adaptive_model(x_val).item()\n",
    "            error_pt = abs(y_true_pt - y_pred_pt)\n",
    "            rel_error_pt = error_pt / abs(y_true_pt) * 100 if abs(y_true_pt) > 1e-10 else 0\n",
    "            \n",
    "            print(f\"{x_val.item():<6.1f} {y_true_pt:<12.6f} {y_pred_pt:<12.6f} \"\n",
    "                  f\"{error_pt:<12.2e} {rel_error_pt:<10.3f}%\")\n",
    "    \n",
    "    # Check initial condition satisfaction\n",
    "    with torch.no_grad():\n",
    "        y0_pred = adaptive_model(torch.tensor([[0.0]], device=device))\n",
    "        ic_error = abs(y0_pred.item() - 1.0)\n",
    "    \n",
    "    print(f\"\\nINITIAL CONDITION CHECK:\")\n",
    "    print(f\"  y(0) = 1.0 (required)\")\n",
    "    print(f\"  y(0) = {y0_pred.item():.8f} (predicted)\")\n",
    "    print(f\"  Error: {ic_error:.2e}\")\n",
    "    print(f\"  Status: {'✓ SATISFIED' if ic_error < 1e-6 else '⚠ NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    # Physical behavior check\n",
    "    print(f\"\\nPHYSICAL BEHAVIOR VALIDATION:\")\n",
    "    \n",
    "    # Check if solution is decreasing (as expected for y' + y = 0)\n",
    "    x_check = torch.linspace(0, 5, 100).view(-1, 1).to(device)\n",
    "    x_check.requires_grad_(True)\n",
    "    y_check = adaptive_model(x_check)\n",
    "    dy_dx = torch.autograd.grad(y_check, x_check, \n",
    "                               grad_outputs=torch.ones_like(y_check),\n",
    "                               create_graph=False)[0]\n",
    "    \n",
    "    is_decreasing = torch.all(dy_dx < 0.1)  # Allow small numerical errors\n",
    "    print(f\"  Solution monotonically decreasing: {'✓ YES' if is_decreasing else '✗ NO'}\")\n",
    "    print(f\"  Maximum derivative: {torch.max(dy_dx).item():.2e} (should be negative)\")\n",
    "    \n",
    "    # ODE satisfaction check\n",
    "    ode_residual = dy_dx + y_check\n",
    "    max_residual = torch.max(torch.abs(ode_residual)).item()\n",
    "    mean_residual = torch.mean(torch.abs(ode_residual)).item()\n",
    "    \n",
    "    print(f\"  ODE residual (dy/dx + y):\")\n",
    "    print(f\"    Maximum |residual|: {max_residual:.2e}\")\n",
    "    print(f\"    Mean |residual|:    {mean_residual:.2e}\")\n",
    "    print(f\"    ODE satisfaction:   {'✓ EXCELLENT' if max_residual < 1e-6 else '✓ GOOD' if max_residual < 1e-4 else '⚠ NEEDS IMPROVEMENT'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Overall assessment\n",
    "    overall_success = (\n",
    "        l2_relative.item() < 1e-6 and\n",
    "        ic_error < 1e-6 and\n",
    "        max_residual < 1e-6\n",
    "    )\n",
    "    \n",
    "    print(f\"OVERALL ASSESSMENT: {'🎉 OUTSTANDING SUCCESS!' if overall_success else '✓ Good performance, minor improvements possible'}\")\n",
    "    \n",
    "    if overall_success:\n",
    "        print(\"The two-stage optimization achieved machine precision accuracy!\")\n",
    "    \n",
    "    return {\n",
    "        'l2_error': l2_error.item(),\n",
    "        'l2_relative': l2_relative.item(),\n",
    "        'linf_error': l_inf_error.item(),\n",
    "        'ic_error': ic_error,\n",
    "        'max_residual': max_residual,\n",
    "        'success': overall_success\n",
    "    }\n",
    "\n",
    "# Demonstrate final solution quality\n",
    "final_metrics = demonstrate_solution_quality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Best Practices\n",
    "\n",
    "### Key Findings from Two-Stage Optimization:\n",
    "\n",
    "1. **Adam Exploration Phase:**\n",
    "   - Excellent for navigating complex, non-convex loss landscapes\n",
    "   - Adaptive learning rates handle different gradient magnitudes\n",
    "   - Robust to initialization and hyperparameter choices\n",
    "   - Gets \"close\" to optimal solution quickly\n",
    "\n",
    "2. **L-BFGS Refinement Phase:**\n",
    "   - Superior final convergence precision\n",
    "   - Leverages second-order information (curvature)\n",
    "   - Can achieve machine precision accuracy\n",
    "   - Most effective when starting from a good initial point\n",
    "\n",
    "3. **Two-Stage Benefits:**\n",
    "   - Combines exploration capability with precision\n",
    "   - Often achieves 10-100x better final accuracy\n",
    "   - Reliable convergence across different problems\n",
    "   - Computational overhead of L-BFGS is justified by accuracy gains\n",
    "\n",
    "### Implementation Best Practices:\n",
    "\n",
    "1. **Switch Timing:**\n",
    "   - Monitor loss plateaus (e.g., no improvement for 500-1000 epochs)\n",
    "   - Switch when loss drops below problem-specific threshold\n",
    "   - Use learning rate decay as indicator of Adam exhaustion\n",
    "\n",
    "2. **Hyperparameter Guidelines:**\n",
    "   - Adam LR: Start with 1e-3, use scheduler for adaptation\n",
    "   - L-BFGS: Use default parameters, focus on tolerance settings\n",
    "   - Patience: 500-2000 epochs depending on problem complexity\n",
    "\n",
    "3. **Monitoring and Diagnostics:**\n",
    "   - Track both loss and solution accuracy metrics\n",
    "   - Monitor gradient norms and parameter updates\n",
    "   - Validate physics constraint satisfaction\n",
    "\n",
    "### When to Use Two-Stage Optimization:\n",
    "- High-accuracy requirements (engineering applications)\n",
    "- Complex PDE problems with multiple constraints\n",
    "- Inverse problems requiring parameter precision\n",
    "- When computational budget allows for longer training\n",
    "\n",
    "This PyTorch implementation provides full control over the optimization process, enabling researchers to adapt the strategy for their specific PINN applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}